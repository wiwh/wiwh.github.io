[
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "test"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website.",
    "section": "",
    "text": "Under construction.\nThis is my website."
  },
  {
    "objectID": "latent-book.html",
    "href": "latent-book.html",
    "title": "Unobservable",
    "section": "",
    "text": "Unobservable: latent variable models for multivariate data. Freely accessible here."
  },
  {
    "objectID": "posts/001_vae/vae.html",
    "href": "posts/001_vae/vae.html",
    "title": "VAE Implementation for Nonlinear Latent Variable Modeling",
    "section": "",
    "text": "This is variational auto-encoder (VAE) implementation using tensorflow of the following non-linear latent variable model with one latent variable:\n\\[\\begin{align*}\n    Z &\\sim N(0, 1)\\\\\n    X_j | Z &= g_j(Z) + \\epsilon_j\\\\\n    \\epsilon_j &\\sim N(0, \\sigma_j^2), \\quad j=1, \\dots, p\n\\end{align*}\\]\nThis model can be likened to a non-linar factor analysis model, or, with the assumption that \\(\\sigma_j = \\sigma \\quad \\forall j\\), to a one-dimensional non-linear probabilistic principal component analysis (PPCA) model.\nGiven some observed data \\(\\{X^{(i)}\\}_{i=1}^n\\) with \\(X^{(i)} \\in \\mathbb R^p\\), the goal is to estimate the functions \\(g_j: \\mathbb R \\to \\mathbb R\\).\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers"
  },
  {
    "objectID": "posts/001_vae/vae.html#load-data",
    "href": "posts/001_vae/vae.html#load-data",
    "title": "VAE Implementation for Nonlinear Latent Variable Modeling",
    "section": "Load data",
    "text": "Load data\nWe load a toy dataset with dimensions \\(n=100, p=100\\).\n\ndata = pd.read_csv(\"./data/q1/data_1_200_100_1.csv\")\ndata = np.array(data, dtype='float32')\ndata = np.expand_dims(data, axis=-1)\ndata_train = data[:int(data.shape[0]*.9)]\ndata_test  = data[int(data.shape[0]*.9):]\n\n\ndata_train.shape\n\n(180, 100, 1)"
  },
  {
    "objectID": "posts/001_vae/vae.html#create-the-sampling-layer",
    "href": "posts/001_vae/vae.html#create-the-sampling-layer",
    "title": "VAE Implementation for Nonlinear Latent Variable Modeling",
    "section": "Create the sampling layer",
    "text": "Create the sampling layer\n\nclass Sampling(layers.Layer):\n    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
  },
  {
    "objectID": "posts/001_vae/vae.html#create-the-encoder",
    "href": "posts/001_vae/vae.html#create-the-encoder",
    "title": "VAE Implementation for Nonlinear Latent Variable Modeling",
    "section": "Create the encoder",
    "text": "Create the encoder\nThe encoder encodes the observed variables into the parameters of the posterior distribution of \\(Z^{(i)}|Y^{(i)}\\). Both the mean and variance share the first layer.\n\nlatent_dim = 1\n\nencoder_inputs = keras.Input(shape=data_train.shape[1:])\nx = layers.Flatten()(encoder_inputs)\nx = layers.Dense(100, activation=\"relu\")(x)\nz_mean = layers.Dense(100, activation=\"relu\")(x)\nz_mean = layers.Dense(100, activation=\"relu\")(x)\nz_mean =layers.Dense(latent_dim, name=\"z_mean\", activation=\"linear\")(z_mean)\n\nz_log_var = layers.Dense(100, activation=\"relu\")(x)\nz_log_var = layers.Dense(100, activation=\"relu\")(x)\nz_log_var = layers.Dense(latent_dim, name=\"z_log_var\", activation=\"linear\")(z_log_var)\nz = Sampling()([z_mean, z_log_var])\n\nencoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n\nencoder.summary()\n\nModel: \"encoder\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 100, 1)]     0           []                               \n                                                                                                  \n flatten (Flatten)              (None, 100)          0           ['input_1[0][0]']                \n                                                                                                  \n dense (Dense)                  (None, 100)          10100       ['flatten[0][0]']                \n                                                                                                  \n dense_2 (Dense)                (None, 100)          10100       ['dense[0][0]']                  \n                                                                                                  \n dense_4 (Dense)                (None, 100)          10100       ['dense[0][0]']                  \n                                                                                                  \n z_mean (Dense)                 (None, 1)            101         ['dense_2[0][0]']                \n                                                                                                  \n z_log_var (Dense)              (None, 1)            101         ['dense_4[0][0]']                \n                                                                                                  \n sampling (Sampling)            (None, 1)            0           ['z_mean[0][0]',                 \n                                                                  'z_log_var[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 30,502\nTrainable params: 30,502\nNon-trainable params: 0\n__________________________________________________________________________________________________"
  },
  {
    "objectID": "posts/001_vae/vae.html#create-the-decoder",
    "href": "posts/001_vae/vae.html#create-the-decoder",
    "title": "VAE Implementation for Nonlinear Latent Variable Modeling",
    "section": "Create the Decoder",
    "text": "Create the Decoder\nThe decoder has to be flexible enough to be able to model the functions \\(g_j\\), which models the conditional means of the responses. On top of that, the decoder models the residual variances \\(\\sigma_j\\).\n\nlatent_inputs = keras.Input(shape=(latent_dim,))\nx = layers.Dense(50, activation=\"relu\")(latent_inputs)\nx = layers.Dense(100, activation=\"relu\")(x)\n\nlog_var = layers.Dense(100, activation=\"relu\")(x)\nlog_var = layers.Dense(100, activation=\"linear\")(log_var)\nlog_var = layers.Reshape(data_train.shape[1:])(log_var)\n\n\nx_output = layers.Dense(100, activation=\"relu\")(x)\nx_output = layers.Dense(100, activation=\"linear\")(x_output)\nx_output = layers.Reshape(data_train.shape[1:])(x_output)\n\ndecoder = keras.Model(latent_inputs, [x_output, log_var], name=\"decoder\")\ndecoder.summary()\n\nModel: \"decoder\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 1)]          0           []                               \n                                                                                                  \n dense_5 (Dense)                (None, 50)           100         ['input_2[0][0]']                \n                                                                                                  \n dense_6 (Dense)                (None, 100)          5100        ['dense_5[0][0]']                \n                                                                                                  \n dense_9 (Dense)                (None, 100)          10100       ['dense_6[0][0]']                \n                                                                                                  \n dense_7 (Dense)                (None, 100)          10100       ['dense_6[0][0]']                \n                                                                                                  \n dense_10 (Dense)               (None, 100)          10100       ['dense_9[0][0]']                \n                                                                                                  \n dense_8 (Dense)                (None, 100)          10100       ['dense_7[0][0]']                \n                                                                                                  \n reshape_1 (Reshape)            (None, 100, 1)       0           ['dense_10[0][0]']               \n                                                                                                  \n reshape (Reshape)              (None, 100, 1)       0           ['dense_8[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 45,600\nTrainable params: 45,600\nNon-trainable params: 0\n__________________________________________________________________________________________________"
  },
  {
    "objectID": "posts/001_vae/vae.html#create-the-variational-autoencoder",
    "href": "posts/001_vae/vae.html#create-the-variational-autoencoder",
    "title": "VAE Implementation for Nonlinear Latent Variable Modeling",
    "section": "Create the variational autoencoder",
    "text": "Create the variational autoencoder\nThe variational auto-encoder (VAE) is itself a keras.Model object. It consists of the encoder and decoder layers, a specialized train_step as well of some specialized metrics to keep track of our progress.\n\nclass VAE(keras.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super().__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = keras.metrics.Mean(\n            name=\"reconstruction_loss\"\n        )\n        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, z = self.encoder(data)\n            reconstruction, logvar = self.decoder(z)\n            logvar = tf.reduce_mean(logvar, axis=1, keepdims=True)\n\n            # Reconstruction loss (including the residual variances)\n            reconstruction_loss = 0.5 * tf.reduce_mean(tf.reduce_sum((data - reconstruction)**2 * tf.math.exp(-logvar) + logvar + tf.math.log(2. * np.pi), axis=1))\n\n            # ELBO loss, approximating the posterior with a Gaussian.\n            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n            total_loss = reconstruction_loss + kl_loss\n\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n        return {\n            \"total_loss\": self.total_loss_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n            \"kl_loss\": self.kl_loss_tracker.result(),\n        }\n\nNow Markdown\n\nvae = VAE(encoder, decoder)\nvae.compile(optimizer=keras.optimizers.Adam())\nvae.fit(data_train, epochs=200)"
  },
  {
    "objectID": "posts/001_vae/vae.html#plot",
    "href": "posts/001_vae/vae.html#plot",
    "title": "VAE Implementation for Nonlinear Latent Variable Modeling",
    "section": "Plot",
    "text": "Plot\nWe now plot the fit, i.e. the reconstruction vs the original data.\n\nencoded, _, _ = vae.encoder.predict(data_train)\ndecoded, var = vae.decoder.predict(encoded)\n\nf, ax = plt.subplots()\nax.scatter(\n    data_train.reshape((np.prod(decoded.shape),)),\n    decoded.reshape((np.prod(decoded.shape),))\n)\nax.plot([-5,5], [-5,5], 'k-')\nplt.title(\"Reconstructed vs Original data.\")\nplt.xlabel(\"Original data.\")\nplt.ylabel(\"Reconstructed data.\")\nplt.show()\n\n6/6 [==============================] - 0s 3ms/step\n6/6 [==============================] - 0s 2ms/step\n\n\n\n\n\nWe can also plot the estimated latent variables (the encoded data), which should approximately be standard normal random variables.\n\nplt.hist(encoded)\nplt.title(\"Empirical distribution of the estimated latent variables.\")\nplt.xlabel(\"Value of the estimated latent variable.\")\nplt.ylabel(\"Frequency\")\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\nFinally, we can also plot a selection of the \\(g_j\\) functions.\n\n# Define a range of values to plot\nz_grid = np.arange(-3,3, .1)\nz_grid = np.expand_dims(z_grid, axis=1)\n\n# Obtain the map of these values through the estimated g_j functions\ny_grid, _ = vae.decoder.predict(z_grid)\n\n# Plot 20 of these\nn = 20\nplt.figure()\nfor i in range(n):\n    plt.subplot(4,int(n/4),i+1)\n    plt.plot(z_grid[:,-1], y_grid[:,i, -1])\n\n2/2 [==============================] - 0s 5ms/step"
  },
  {
    "objectID": "posts/005_GLLVM-longi/longtinundinal_gaussian_nuisance.html",
    "href": "posts/005_GLLVM-longi/longtinundinal_gaussian_nuisance.html",
    "title": "GLLVM for longitudinal data",
    "section": "",
    "text": "We implement a longitudinal GLLVM model with latent variables which follow an AR(1) process through time.\n\n\nLet \\(y_{i1t},y_{i2t},\\ldots,y_{ipt}\\) be a set of \\(p\\) response or observed variables at time \\(t,\\ t=1,\\ldots,T\\) for individual \\(i,\\ i=1,\\ldots,n\\). Let \\(\\mathbf{x}_{it}\\) be a set of observed \\(k\\)-dimensional covariates at time \\(t,\\ t=1,\\ldots,T\\).\nModels for multivariate longitudinal data have to account for the three sources of variability present in the data, that is (i) cross-sectional associations between the responses at a particular time point, (ii) cross-lagged associations between different responses at different occasions, and (iii) the association between repeated measures of the same response over time. The first source of variability is accounted for a time-dependent latent variable \\(z_{i1}, z_{i2},\\ldots,z_{iT}\\). Modeling the temporal evolution of the latent variable accounts for the cross-lagged associations between different responses over time. The third source of variability can be accounted for a set of item-specific random effects \\(\\mathbf{u}_{i}=(u_{i1}, \\ldots, u_{ip})'\\).\nAccording to the GLLVM framework we have\n\\[\\begin{align*}\n   \\nonumber y_{ijt}|\\mu_{ijt} &\\sim \\mathcal{F}_j(y_{ijt}\\vert \\mu_{ijt}, \\tau_j)\\\\\n   \\mu_{ijt}&=  g_j(\\eta_{ijt})=g_j(\\beta_{0jt} + \\mathbf{x}_{i}^{\\top}\\boldsymbol \\beta_{jt} + z_{it}^{\\top}\\lambda_{jt}+u_{ij}\\sigma_{u_j})\\\\ %  \\label{eqn:GLLVM-model2}\n\\end{align*}\\] where \\(g_j(\\cdot),j=1,\\ldots,p\\) is a known {}, \\(\\eta_{ijt}=\\beta_{0jt} + \\mathbf{x}_{i}^{\\top}\\boldsymbol \\beta_{jt} + z_{it}^{\\top}\\lambda_{jt}+u_{ij},i=1,\\ldots,n,j=1,\\ldots,p, t=1,\\ldots,T\\) is the {}, and \\(\\mathcal{F}_j(y_{ijt}\\vert \\eta_{ijt}, \\tau_j)\\) denotes a distribution from the exponential family with mean \\(\\mu_{ijt}\\) and response variable-specific dispersion parameter \\(\\tau_j\\). \\ The dynamics of the latent variable over time is modelled through a non-stationary autoregressive model of first order\n\\[\\begin{equation*}\nz_{it}=\\phi z_{i,t-1} + \\delta_{it}\n\\end{equation*}\\] where \\(z_{i1}\\sim N(0,\\sigma^2_{1})\\) and \\(\\delta_{it}\\sim N(0,1)\\). Moreover, we assume the random effects independent of the latent variable and their common distribution \\(\\mathbf{u}_{i}\\sim N_p(\\mathbf{0}, \\boldsymbol I)\\).\n\n\n\nThe latent variable \\(z_{it}\\) has to be the same (same meaning) across occasions. Thus the measurement invariance assumption has to be tested on the data, that is all the measurement parameters are invariant across occasions, that is \\[\\beta_{0jt}=\\beta_{0j} \\ \\textrm{and } \\ \\lambda_{jt}=\n\\lambda_{j},\\] for all \\(t\\), \\(t=1, \\ldots, T\\) and for all \\(j\\), \\(j=1,\\ldots, p\\). Under this assumption, the model is more parsimonious and avoids some possible identification problem that might arise with increasing the number of time points.\nTo ensure identification of the model, one necessary condition is that the latent variable has a scale and an origin. %When measurement %invariance of loadings and intercepts is imposed, Scale for \\(z_{it}\\) can be provided either by fixing one loading at a nonzero value or by fixing the factor variance at a nonzero value. In presence of longitudinal data, the same loading is fixed equal to one at each occasion.\n\n\n\nWe model each observation as a tuple of dimension (T, p), common across individuals. Individuals constitute independent observations, which yields the tensor structure (n, T, q). The time dimension T appears in the first dimension since it allows for seamless tensor products of the type (n, T, q) (q, p).\nElements we need:\n\nA model that defines the generative process\nA loss function which, upon taking the derivative, re-creates the estimating equations\nA way to compute the latent variables. For now, we will use a neural network. Later, we can implement the real function.\n\n\nimport torch\nfrom torch import nn\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\nn = 100\np = 15\nT = 20\nk = 8\nq = 1\n\nDIMENSIONS_Y = (n, T, p) \nDIMENSIONS_X = (n, T, k)\nDIMENSIONS_Z = (n, T, q)\nDIMENSIONS_U = (n, 1, p)\n\n# TODO: refactor to have a module of two modules: encoder and decoder\nclass GLLVM_longitudinal():\n    def __init__(self):\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n\n        # nuisance parameters\n        self.phi = torch.ones(1) * .3\n        self.var_u = torch.ones((1,1,p))\n        self.var_y = torch.ones((1,1,p))\n        self.var_z = torch.ones((1,T,1))\n\n    def encoder_fit(self, x, y, z, u, epochs=100, verbose=False):\n        encoder_loss = nn.MSELoss()\n        encoder_opt = torch.optim.Adam(self.encoder.parameters())\n        for epoch in range(epochs):\n            (zhat, uhat) = self.encoder(x, y)\n            loss = self.encoder_loss(zhat, z) + encoder_loss(uhat, u)\n            if verbose:\n                print(f\"\\nEpoch {epoch}/{epochs}, loss={loss}\")\n            loss.backward()\n            encoder_opt.step()\n            encoder_opt.zero_grad()\n        return loss\n\n    def encoder_loss(self, input_z, target_z):\n        # here we penalize the variances too !!\n        phi_input = (torch.sum(input_z[:,1:]*input_z[:,:(T-1)], dim=1) / torch.sum(torch.pow(input_z[:,:(T-1)],2), dim=1)).mean()\n        phi_target = (torch.sum(target_z[:,1:]*target_z[:,:(T-1)], dim=1) / torch.sum(torch.pow(target_z[:,:(T-1)],2), dim=1)).mean()\n        return torch.sum(pow(input_z-target_z, 2), dim=[1,2]).mean() + 1e4*pow(phi_input-phi_target, 2)\n\n    def update_nuisance_parameters(self, z_sample, u_sample, z_sim, u_sim, lr=1e-1):\n        # update phi:\n        # phi_sample = torch.sum(z_sample[:,1:]*z_sample[:,:(T-1)], dim=1) # - self.phi* torch.sum(torch.pow(z_sample[:,:(T-1)],2), dim=1)\n        # phi_sim = torch.sum(z_sim[:,1:]*z_sim[:,:(T-1)], dim=1) #- self.phi * torch.sum(torch.pow(z_sim[:,:(T-1)],2), dim=1)\n        # self.phi = self.phi + lr * (math.sqrt(phi_sample.mean()) - math.sqrt(phi_sim.mean()))\n        phi_sample = torch.sum(z_sample[:,1:]*z_sample[:,:(T-1)], dim=1) / torch.sum(torch.pow(z_sample[:,:(T-1)],2), dim=1)\n        phi_sim = torch.sum(z_sim[:,1:]*z_sim[:,:(T-1)], dim=1) / torch.sum(torch.pow(z_sim[:,:(T-1)],2), dim=1)\n        # var_u = torch.mean(torch.pow(u_sample, 2), dim=0)\n\n        self.phi = self.phi  + lr * (phi_sample.mean() - phi_sim.mean())\n        #self.var_u = self.var_u * (1-lr) + lr * var_u\n\n\n    def sample(self, n, x=None, z=None, u=None, d=None):\n        \"\"\"Sample a longitudinal GLLVM, potentially with z, u, and d(elta), and return (x, z, u, d, y)\"\"\"\n\n        with torch.no_grad():\n            if x is None:\n                Warning(\"xb was set to None for sampling. This is usually unwanted unless k=0.\")\n                x = torch.randn((n, T, k))\n            if z is None:\n                z = torch.randn((n, T, q)) * torch.sqrt(self.var_z)\n\n            if d is None:\n                d = torch.randn((n, T, q))\n        \n            if u is None:\n                u = torch.randn((n, 1, p)) * torch.sqrt(self.var_u)\n        \n            z = self.AR(z, d)\n\n            eps = torch.randn((n, T, p)) * torch.sqrt(self.var_y)\n            y = self.decoder(x, z, u) + eps\n\n        return {\"x\":x, \"z\":z, \"u\":u, \"d\":d, \"y\":y}\n    \n    def AR(self, z, delta):\n        assert z.shape == delta.shape  # we draw the same shape for simplicity, even though we don't need delta for t=0.\n        for t in range(1, z.shape[1]):\n            z[:,t] = z[:, t-1] * self.phi + delta[:,t]\n        return z\n\n    def sample_z(self):\n        return None\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # decoder part (our parameters of interest)\n        self.wz = nn.Parameter(torch.randn((q, p)))\n        self.wx = nn.Parameter(torch.randn((T, k, p)))\n        self.bias = torch.zeros((1, T, p))\n\n    # decoding\n    def forward(self, x, z, u):\n        xwx = (x.unsqueeze(2) @ self.wx).squeeze() # see details of tensorproducts\n        zwz = (z.unsqueeze(2) @ self.wz).squeeze()\n        linpar = self.bias + xwx + zwz + u \n        return linpar\n\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # encoder part\n        # input dimension is T * (p+k)... we buuild a fully connected layer but it isn't necessary \n        # output dimension is T*q  + p (for Z and U, respectively)\n        self.enc_model = nn.Sequential(\n            nn.Linear(in_features=T*(p+k), out_features = 100),\n            nn.ReLU(),\n            nn.Linear(in_features=100, out_features = 100),\n            nn.ReLU(),\n            nn.Linear(in_features=100, out_features = 100),\n            nn.ReLU(),\n            nn.Linear(in_features=100, out_features = T*q + p)\n        )\n    \n    def forward(self, x, y):\n        xy = torch.cat([x, y], dim=2).flatten(start_dim=1)\n        zu = self.enc_model(xy)\n        return self.split_zu(zu)\n\n    def split_zu(self, zu):\n        #output dimension of size (T*Z), p\n        z, u = torch.split(zu, [T*q, p], dim=1)\n        z = z.reshape((z.shape[0], T, q))\n        u = u.unsqueeze(1)\n        return (z, u)\n\nclass Nuisance(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.enc_model = nn.Sequential(\n            nn.Linear(in_features=(T-1)*q + p, out_features = 50), # T-1 because we devide T+1 by T (feature engineering)\n            nn.ReLU(),\n            nn.Linear(in_features=50, out_features = 50),\n            nn.ReLU(),\n            # phi is 1 dimensional, var_u is p-dimensional, var_y is p dimensional (for non-singleton dimensions)\n            nn.Linear(1 + p + p)\n        )\n    \n    def forward(self):\n        None\n\n\n\n\ngl_true = GLLVM_longitudinal()\ndat_true = gl_true.sample(n)\n\ngl = GLLVM_longitudinal()\n\n\ndef my_loss(input, target, sign=1):\n    return sign * torch.sum(input*target, dim=[1,2]).mean()\n\n\n\n\ndef evaluate_fit(input, target):\n    with torch.no_grad():\n        return torch.sum(torch.pow(input - target,2), dim=[1,2]).mean()\n\n\nwith torch.no_grad():\n    dat_sim = gl.sample(n, x=dat_true[\"x\"]) # x are known and fixed\n\n# train the encoder\n\ngl.encoder_fit(dat_sim[\"x\"], dat_sim[\"y\"], dat_sim[\"z\"], dat_sim[\"u\"], epochs=100)\n\ntensor(23.1854, grad_fn=&lt;AddBackward0&gt;)\n\n\n\ndecoder_opt = torch.optim.SGD(gl.decoder.parameters(), lr=1e-2)\n# decoder_opt = torch.optim.Adam(gl.decoder.parameters())\n\nepochs = 2000\nlr_init = 1e-3\nfor epoch in range(1, epochs+1):\n    lr= lr_init#/math.sqrt(epochs)\n    \n    with torch.no_grad():\n        dat_sim = gl.sample(n, x=dat_true[\"x\"]) # x are known and fixed\n\n    # train the encoder\n\n    encoder_loss = gl.encoder_fit(dat_sim[\"x\"], dat_sim[\"y\"], dat_sim[\"z\"], dat_sim[\"u\"], epochs=10)\n    \n    # compute SPRIME sample step\n\n    # compute imputing values\n    with torch.no_grad():\n        zhat_true, uhat_true = gl.encoder(dat_true[\"x\"], dat_true[\"y\"])\n\n    linpar_sample = gl.decoder(dat_true[\"x\"], zhat_true, uhat_true)\n    loss = my_loss(linpar_sample, dat_true[\"y\"], sign=-1) #notice the -sign here\n    loss.backward()\n\n    # compute SPRIME simulation step\n    # compute imputing values\n    with torch.no_grad():\n        zhat_sim, uhat_sim = gl.encoder(dat_sim[\"x\"], dat_sim[\"y\"])\n        \n    linpar_sim = gl.decoder(dat_sim[\"x\"], zhat_sim, uhat_sim)\n    loss = my_loss(linpar_sim, dat_sim[\"y\"], sign=1)\n    loss.backward()\n\n    with torch.no_grad():\n        for par in gl.decoder.parameters():\n            par -= par.grad * lr\n        gl.decoder.zero_grad()\n\n    # Update nuisance parameters\n    with torch.no_grad():\n        gl.update_nuisance_parameters(zhat_true, uhat_true, zhat_sim, uhat_sim, lr=1e-2)\n\n    # evaluate the model\n    if epoch == 1 or epoch % 10 == 0:\n        with torch.no_grad():\n            loss = evaluate_fit(linpar_sample, dat_true[\"y\"])\n\n        print(f\"\\nEpoch {epoch}/{epochs}, loss = {loss:.2f}, encoder_loss = {encoder_loss:.2f}, phi= {gl.phi}, var_u ={gl.var_u[0,0,0]}\")\n\n\nwith torch.no_grad():\n    zhat, uhat = gl.encoder(dat_true[\"x\"], dat_true[\"y\"])\n    yhat = gl.decoder(dat_true[\"x\"], zhat, uhat)\n\nplt.scatter(dat_true[\"y\"], yhat)\nplt.plot([-20, 20], [-20, 20], color=\"red\")\n\n\n\n\n\nzhat = zhat*-1\nplt.scatter(zhat, dat_true[\"z\"])\nplt.plot([-10, 10], [-10,10], color=\"red\")\n\n\n\n\n\n# print 12 randomly selected Z\nindex = np.random.choice(range(n), 12, replace=False)\nfig, axs = plt.subplots(3, 4)\n\nzhat= -zhat\nfig.suptitle(\"Ztrue vs Zest across time\")\nfor i in range(12):\n    axs[i//4, i%4].plot(zhat[index[i],:,0]*-1)\n    axs[i//4, i%4].plot(dat_true[\"z\"][index[i],:, 0], color=\"red\")\n\n\n\n\n\npar_true = gl_true.decoder.parameters().__next__().detach().squeeze()\npar_est = gl.decoder.parameters().__next__() .detach().squeeze()\n\n\nplt.scatter(par_true, par_est)\nplt.plot([-2,2], [2, -2])\n\n\n\n\n\n\nWe now show the details on the tensor products, for instance for computing xb @ wx. xb is of size (n, T, q) and wx is of size (T, q, p). We want a result of size (n, T, p). First we add a dimension for xb:\nxb.unsqueeze(2) which yields a dimensions of (n, T, 1, q)\nwhich we then multiply by wz:\n(n, T, 1, q) @ (1, q, p) -&gt; (n, T, 1, p)\nwhere the first dimension of wx has been broadcasted.\nFinally, we squeeze to obtain (n, T, p)."
  },
  {
    "objectID": "posts/005_GLLVM-longi/longtinundinal_gaussian_nuisance.html#measurement-invariance",
    "href": "posts/005_GLLVM-longi/longtinundinal_gaussian_nuisance.html#measurement-invariance",
    "title": "GLLVM for longitudinal data",
    "section": "",
    "text": "The latent variable \\(z_{it}\\) has to be the same (same meaning) across occasions. Thus the measurement invariance assumption has to be tested on the data, that is all the measurement parameters are invariant across occasions, that is \\[\\beta_{0jt}=\\beta_{0j} \\ \\textrm{and } \\ \\lambda_{jt}=\n\\lambda_{j},\\] for all \\(t\\), \\(t=1, \\ldots, T\\) and for all \\(j\\), \\(j=1,\\ldots, p\\). Under this assumption, the model is more parsimonious and avoids some possible identification problem that might arise with increasing the number of time points.\nTo ensure identification of the model, one necessary condition is that the latent variable has a scale and an origin. %When measurement %invariance of loadings and intercepts is imposed, Scale for \\(z_{it}\\) can be provided either by fixing one loading at a nonzero value or by fixing the factor variance at a nonzero value. In presence of longitudinal data, the same loading is fixed equal to one at each occasion."
  },
  {
    "objectID": "posts/005_GLLVM-longi/longtinundinal_gaussian_nuisance.html#implementation-using-pytorch",
    "href": "posts/005_GLLVM-longi/longtinundinal_gaussian_nuisance.html#implementation-using-pytorch",
    "title": "GLLVM for longitudinal data",
    "section": "",
    "text": "We model each observation as a tuple of dimension (T, p), common across individuals. Individuals constitute independent observations, which yields the tensor structure (n, T, q). The time dimension T appears in the first dimension since it allows for seamless tensor products of the type (n, T, q) (q, p).\nElements we need:\n\nA model that defines the generative process\nA loss function which, upon taking the derivative, re-creates the estimating equations\nA way to compute the latent variables. For now, we will use a neural network. Later, we can implement the real function.\n\n\nimport torch\nfrom torch import nn\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\nn = 100\np = 15\nT = 20\nk = 8\nq = 1\n\nDIMENSIONS_Y = (n, T, p) \nDIMENSIONS_X = (n, T, k)\nDIMENSIONS_Z = (n, T, q)\nDIMENSIONS_U = (n, 1, p)\n\n# TODO: refactor to have a module of two modules: encoder and decoder\nclass GLLVM_longitudinal():\n    def __init__(self):\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n\n        # nuisance parameters\n        self.phi = torch.ones(1) * .3\n        self.var_u = torch.ones((1,1,p))\n        self.var_y = torch.ones((1,1,p))\n        self.var_z = torch.ones((1,T,1))\n\n    def encoder_fit(self, x, y, z, u, epochs=100, verbose=False):\n        encoder_loss = nn.MSELoss()\n        encoder_opt = torch.optim.Adam(self.encoder.parameters())\n        for epoch in range(epochs):\n            (zhat, uhat) = self.encoder(x, y)\n            loss = self.encoder_loss(zhat, z) + encoder_loss(uhat, u)\n            if verbose:\n                print(f\"\\nEpoch {epoch}/{epochs}, loss={loss}\")\n            loss.backward()\n            encoder_opt.step()\n            encoder_opt.zero_grad()\n        return loss\n\n    def encoder_loss(self, input_z, target_z):\n        # here we penalize the variances too !!\n        phi_input = (torch.sum(input_z[:,1:]*input_z[:,:(T-1)], dim=1) / torch.sum(torch.pow(input_z[:,:(T-1)],2), dim=1)).mean()\n        phi_target = (torch.sum(target_z[:,1:]*target_z[:,:(T-1)], dim=1) / torch.sum(torch.pow(target_z[:,:(T-1)],2), dim=1)).mean()\n        return torch.sum(pow(input_z-target_z, 2), dim=[1,2]).mean() + 1e4*pow(phi_input-phi_target, 2)\n\n    def update_nuisance_parameters(self, z_sample, u_sample, z_sim, u_sim, lr=1e-1):\n        # update phi:\n        # phi_sample = torch.sum(z_sample[:,1:]*z_sample[:,:(T-1)], dim=1) # - self.phi* torch.sum(torch.pow(z_sample[:,:(T-1)],2), dim=1)\n        # phi_sim = torch.sum(z_sim[:,1:]*z_sim[:,:(T-1)], dim=1) #- self.phi * torch.sum(torch.pow(z_sim[:,:(T-1)],2), dim=1)\n        # self.phi = self.phi + lr * (math.sqrt(phi_sample.mean()) - math.sqrt(phi_sim.mean()))\n        phi_sample = torch.sum(z_sample[:,1:]*z_sample[:,:(T-1)], dim=1) / torch.sum(torch.pow(z_sample[:,:(T-1)],2), dim=1)\n        phi_sim = torch.sum(z_sim[:,1:]*z_sim[:,:(T-1)], dim=1) / torch.sum(torch.pow(z_sim[:,:(T-1)],2), dim=1)\n        # var_u = torch.mean(torch.pow(u_sample, 2), dim=0)\n\n        self.phi = self.phi  + lr * (phi_sample.mean() - phi_sim.mean())\n        #self.var_u = self.var_u * (1-lr) + lr * var_u\n\n\n    def sample(self, n, x=None, z=None, u=None, d=None):\n        \"\"\"Sample a longitudinal GLLVM, potentially with z, u, and d(elta), and return (x, z, u, d, y)\"\"\"\n\n        with torch.no_grad():\n            if x is None:\n                Warning(\"xb was set to None for sampling. This is usually unwanted unless k=0.\")\n                x = torch.randn((n, T, k))\n            if z is None:\n                z = torch.randn((n, T, q)) * torch.sqrt(self.var_z)\n\n            if d is None:\n                d = torch.randn((n, T, q))\n        \n            if u is None:\n                u = torch.randn((n, 1, p)) * torch.sqrt(self.var_u)\n        \n            z = self.AR(z, d)\n\n            eps = torch.randn((n, T, p)) * torch.sqrt(self.var_y)\n            y = self.decoder(x, z, u) + eps\n\n        return {\"x\":x, \"z\":z, \"u\":u, \"d\":d, \"y\":y}\n    \n    def AR(self, z, delta):\n        assert z.shape == delta.shape  # we draw the same shape for simplicity, even though we don't need delta for t=0.\n        for t in range(1, z.shape[1]):\n            z[:,t] = z[:, t-1] * self.phi + delta[:,t]\n        return z\n\n    def sample_z(self):\n        return None\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # decoder part (our parameters of interest)\n        self.wz = nn.Parameter(torch.randn((q, p)))\n        self.wx = nn.Parameter(torch.randn((T, k, p)))\n        self.bias = torch.zeros((1, T, p))\n\n    # decoding\n    def forward(self, x, z, u):\n        xwx = (x.unsqueeze(2) @ self.wx).squeeze() # see details of tensorproducts\n        zwz = (z.unsqueeze(2) @ self.wz).squeeze()\n        linpar = self.bias + xwx + zwz + u \n        return linpar\n\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # encoder part\n        # input dimension is T * (p+k)... we buuild a fully connected layer but it isn't necessary \n        # output dimension is T*q  + p (for Z and U, respectively)\n        self.enc_model = nn.Sequential(\n            nn.Linear(in_features=T*(p+k), out_features = 100),\n            nn.ReLU(),\n            nn.Linear(in_features=100, out_features = 100),\n            nn.ReLU(),\n            nn.Linear(in_features=100, out_features = 100),\n            nn.ReLU(),\n            nn.Linear(in_features=100, out_features = T*q + p)\n        )\n    \n    def forward(self, x, y):\n        xy = torch.cat([x, y], dim=2).flatten(start_dim=1)\n        zu = self.enc_model(xy)\n        return self.split_zu(zu)\n\n    def split_zu(self, zu):\n        #output dimension of size (T*Z), p\n        z, u = torch.split(zu, [T*q, p], dim=1)\n        z = z.reshape((z.shape[0], T, q))\n        u = u.unsqueeze(1)\n        return (z, u)\n\nclass Nuisance(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.enc_model = nn.Sequential(\n            nn.Linear(in_features=(T-1)*q + p, out_features = 50), # T-1 because we devide T+1 by T (feature engineering)\n            nn.ReLU(),\n            nn.Linear(in_features=50, out_features = 50),\n            nn.ReLU(),\n            # phi is 1 dimensional, var_u is p-dimensional, var_y is p dimensional (for non-singleton dimensions)\n            nn.Linear(1 + p + p)\n        )\n    \n    def forward(self):\n        None\n\n\n\n\ngl_true = GLLVM_longitudinal()\ndat_true = gl_true.sample(n)\n\ngl = GLLVM_longitudinal()\n\n\ndef my_loss(input, target, sign=1):\n    return sign * torch.sum(input*target, dim=[1,2]).mean()\n\n\n\n\ndef evaluate_fit(input, target):\n    with torch.no_grad():\n        return torch.sum(torch.pow(input - target,2), dim=[1,2]).mean()\n\n\nwith torch.no_grad():\n    dat_sim = gl.sample(n, x=dat_true[\"x\"]) # x are known and fixed\n\n# train the encoder\n\ngl.encoder_fit(dat_sim[\"x\"], dat_sim[\"y\"], dat_sim[\"z\"], dat_sim[\"u\"], epochs=100)\n\ntensor(23.1854, grad_fn=&lt;AddBackward0&gt;)\n\n\n\ndecoder_opt = torch.optim.SGD(gl.decoder.parameters(), lr=1e-2)\n# decoder_opt = torch.optim.Adam(gl.decoder.parameters())\n\nepochs = 2000\nlr_init = 1e-3\nfor epoch in range(1, epochs+1):\n    lr= lr_init#/math.sqrt(epochs)\n    \n    with torch.no_grad():\n        dat_sim = gl.sample(n, x=dat_true[\"x\"]) # x are known and fixed\n\n    # train the encoder\n\n    encoder_loss = gl.encoder_fit(dat_sim[\"x\"], dat_sim[\"y\"], dat_sim[\"z\"], dat_sim[\"u\"], epochs=10)\n    \n    # compute SPRIME sample step\n\n    # compute imputing values\n    with torch.no_grad():\n        zhat_true, uhat_true = gl.encoder(dat_true[\"x\"], dat_true[\"y\"])\n\n    linpar_sample = gl.decoder(dat_true[\"x\"], zhat_true, uhat_true)\n    loss = my_loss(linpar_sample, dat_true[\"y\"], sign=-1) #notice the -sign here\n    loss.backward()\n\n    # compute SPRIME simulation step\n    # compute imputing values\n    with torch.no_grad():\n        zhat_sim, uhat_sim = gl.encoder(dat_sim[\"x\"], dat_sim[\"y\"])\n        \n    linpar_sim = gl.decoder(dat_sim[\"x\"], zhat_sim, uhat_sim)\n    loss = my_loss(linpar_sim, dat_sim[\"y\"], sign=1)\n    loss.backward()\n\n    with torch.no_grad():\n        for par in gl.decoder.parameters():\n            par -= par.grad * lr\n        gl.decoder.zero_grad()\n\n    # Update nuisance parameters\n    with torch.no_grad():\n        gl.update_nuisance_parameters(zhat_true, uhat_true, zhat_sim, uhat_sim, lr=1e-2)\n\n    # evaluate the model\n    if epoch == 1 or epoch % 10 == 0:\n        with torch.no_grad():\n            loss = evaluate_fit(linpar_sample, dat_true[\"y\"])\n\n        print(f\"\\nEpoch {epoch}/{epochs}, loss = {loss:.2f}, encoder_loss = {encoder_loss:.2f}, phi= {gl.phi}, var_u ={gl.var_u[0,0,0]}\")\n\n\nwith torch.no_grad():\n    zhat, uhat = gl.encoder(dat_true[\"x\"], dat_true[\"y\"])\n    yhat = gl.decoder(dat_true[\"x\"], zhat, uhat)\n\nplt.scatter(dat_true[\"y\"], yhat)\nplt.plot([-20, 20], [-20, 20], color=\"red\")\n\n\n\n\n\nzhat = zhat*-1\nplt.scatter(zhat, dat_true[\"z\"])\nplt.plot([-10, 10], [-10,10], color=\"red\")\n\n\n\n\n\n# print 12 randomly selected Z\nindex = np.random.choice(range(n), 12, replace=False)\nfig, axs = plt.subplots(3, 4)\n\nzhat= -zhat\nfig.suptitle(\"Ztrue vs Zest across time\")\nfor i in range(12):\n    axs[i//4, i%4].plot(zhat[index[i],:,0]*-1)\n    axs[i//4, i%4].plot(dat_true[\"z\"][index[i],:, 0], color=\"red\")\n\n\n\n\n\npar_true = gl_true.decoder.parameters().__next__().detach().squeeze()\npar_est = gl.decoder.parameters().__next__() .detach().squeeze()\n\n\nplt.scatter(par_true, par_est)\nplt.plot([-2,2], [2, -2])\n\n\n\n\n\n\nWe now show the details on the tensor products, for instance for computing xb @ wx. xb is of size (n, T, q) and wx is of size (T, q, p). We want a result of size (n, T, p). First we add a dimension for xb:\nxb.unsqueeze(2) which yields a dimensions of (n, T, 1, q)\nwhich we then multiply by wz:\n(n, T, 1, q) @ (1, q, p) -&gt; (n, T, 1, p)\nwhere the first dimension of wx has been broadcasted.\nFinally, we squeeze to obtain (n, T, p)."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "GLLVM for longitudinal data\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nGuillaum eBlanc\n\n\n\n\n\n\n  \n\n\n\n\nVAE Implementation for Nonlinear Latent Variable Modeling\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\nGuillaum eBlanc\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tidbits/001_oanth/oanth.html",
    "href": "tidbits/001_oanth/oanth.html",
    "title": "Connecting to Strava using oanth2",
    "section": "",
    "text": "test"
  },
  {
    "objectID": "tidbits/002_AR/AR_intro.html",
    "href": "tidbits/002_AR/AR_intro.html",
    "title": "RNN state space models",
    "section": "",
    "text": "Present RNN state space models, in the context of latent-variable modeling, and see if I have some ideas for their estimation."
  },
  {
    "objectID": "tidbits/002_AR/AR_intro.html#the-arp-processes",
    "href": "tidbits/002_AR/AR_intro.html#the-arp-processes",
    "title": "RNN state space models",
    "section": "The AR(p) Processes",
    "text": "The AR(p) Processes\nAn AR(p) process of order \\(p\\) is a time series model where the current value of the variable is a linear combination of the \\(p\\) most recent values of the same variable, plus a random error term. Mathematical representation: The general form of an AR(p) process can be written as:\n\\[\n   Y_t = b + \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + \\ldots + \\phi_pY_{t-p} + \\varepsilon_t\n   \\]\nwhere \\(Y_t\\) represents the value of the variable at time \\(t\\), \\(b\\) is a constant we will call the bias, \\(\\phi_1, \\phi_2, \\ldots, \\phi_p\\) are the autoregressive parameters, \\(\\varepsilon_t\\) is a random error term. A AR(p) can be simply re-written as:\n\\[\nY_t = g_\\theta(Y_{t-1}, Y_{t-2}, \\ldots, Y_{t-p}) + \\varepsilon_t\n\\]\nfor some function \\(g\\) linear in its arguments."
  },
  {
    "objectID": "tidbits/002_AR/AR_intro.html#the-ar1-processes",
    "href": "tidbits/002_AR/AR_intro.html#the-ar1-processes",
    "title": "RNN state space models",
    "section": "The AR(1) Processes",
    "text": "The AR(1) Processes\nAn AR(1) process is thus simply\n\\[\nY_t = b + \\phi_1 Y_{t-1} + \\varepsilon_t\n\\]"
  },
  {
    "objectID": "tidbits/002_AR/AR_intro.html#rnn",
    "href": "tidbits/002_AR/AR_intro.html#rnn",
    "title": "RNN state space models",
    "section": "RNN",
    "text": "RNN\nA RNN\n\nStationarity: For an AR process to be stationary, the autoregressive parameters ((_1, _2, , _p)) must satisfy certain conditions. Specifically, the roots of the characteristic equation must be outside the unit circle.\nAutocorrelation: AR processes exhibit autocorrelation, which means that the values of the variable are correlated with their past values. The autocorrelation function (ACF) of an AR process shows the correlation between observations at different lags.\nModel estimation: Estimating the parameters of an AR(p) process involves techniques such as maximum likelihood estimation (MLE) or least squares estimation (LSE). The estimated parameters can be used to make predictions and generate forecasts for future values of the variable.\nOrder selection: Determining the appropriate order of an AR process ((p) value) is an important step. It can be done using statistical techniques such as the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC), which help choose the optimal order based on the model’s goodness of fit and complexity.\n\nAR processes are widely used in time series analysis and forecasting. They provide a flexible framework for modeling and understanding the behavior of a variable over time based on its own past values."
  },
  {
    "objectID": "tidbits/0032_WSL-conda-environment/WSL.html",
    "href": "tidbits/0032_WSL-conda-environment/WSL.html",
    "title": "Setting up a minimal data-science environment in Windows using WSL (part II)",
    "section": "",
    "text": "Set up a conda environment for data-science\nNow that we have conda installed, let us set up a conda environment called ds (for data-science). Open WSL, then run\n\nconda create -n ds \n\nActivate the environment:\n\nconda activate ds\n\nWhen you now run python3, it will launch the version of the python interpreter that came with anaconda (in the previous step), not the version that comes pre-installed with WSL.\nYou can now install the following basic data-science packages:\n\nconda install numpy scipy matplotlib seaborn scikit-learn pandas Jupyter\n\n\n\n\n\n\n\nInstalling packages through conda forge\n\n\n\nConda official repository only feature a few verified packages. A vast portion of python packages that are otherwise available through pip are installed through community led channel called conda-forge. You can visit their site to learn more about it. To do this, install your other packages package1 and package2 (say), by specifying the conda-forge channel :\n\nconda install -c conda-forge package1 package2\n\n\n\n\nTest your installation:\nWe now test the installation by using the following workflow:\n\nCreate a python script using the Windows text editor of your choice, save it in your ~/Downloads folder.\nExecute the script from Bash.\n\nCreate a new file called test.py in ~/Downloads with the following content:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\n# Generate some random data\nnp.random.seed(42)\nx = np.random.normal(size=100)\ny = np.random.normal(size=100)\n\n# Create a DataFrame from the data\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Plot a scatter plot using Seaborn\nsns.scatterplot(data=df, x='x', y='y')\n\n# Add a regression line using SciPy\nslope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\nplt.plot(x, slope * x + intercept, color='red')\n\n# Set the x and y labels using Matplotlib\nplt.xlabel('X')\nplt.ylabel('Y')\n\n# Show the plot\nplt.show()\n\nand run it with\n\npython3 ~/Downloads/test.py"
  },
  {
    "objectID": "tidbits/003_WSL/WSL.html",
    "href": "tidbits/003_WSL/WSL.html",
    "title": "Setting up a minimal data-science environment in Windows using WSL (part I)",
    "section": "",
    "text": "This is the first part of a series of tidbits to set up a data-science environement in linux alongside windows, using windows subsystem for linux (WSL).\nThis will include conda as a package / environment manager, using the minimal version miniconda, and the following data-science packages installed for the default interpreter.\n\nnumpy\nscipy\nscikit-learn\nmatplotlib\nseaborn\npandas\nJupyter\n\nFor deep learning packages, this comes next.\n\n\nInstall WSL: Open a powershell as administrator and run\n\nwsl --install\n\nCheck the installation and version with\n\nwsl -l -v\n\nTo activate the Bash shell type\n\nwsl\n\nTo exit the shell type\n\nexit\n\nYou can now use Bash to run linux tools and applications. During the installation process, make note of your login and password.\n\n\nYour home directory is accessed using cd ~. Create a Downloads directory and navigate to it:\n\ncd ~\nmkdir Downloads\ncd Downloads\n\nYou can use explorer.exe . to open File Explorer from the current directory.\n\n\n\n\nRun the following code to update the linux distribution you just installed and make sur it is up-to-date:\n\nsudo apt update\nsudo apt upgrade\n\n\n\n\nPython comes pre-installed in WSL. Check the version:\n\npython3 --version\n\nIf somehow it is not installed, you can install it by running\n\nsudo apt install python3\n\n\n\n\nEven though WSL comes with python 3 pre-installed, we will use miniconda’s python distribution, which includes a package manager.\nGo to https://docs.conda.io/en/latest/miniconda.html to download the installer of your choice (listed under Linux). At the time of this writing the latest installer includes Python 3.10, and put it inside your ~/Downloads folder, which is accessible from windows using the above-mentioned explorer.exe . command.\nPlace your downloaded file in this directory.\n\n\n\n\n\n\nCopying files from Windows to WSL\n\n\n\n\n\nWhen you copy a file from Windows to a Linux directory in WSL, Windows may add a Zone.Identifier file to the directory containing the copied file. This is because Windows considers the Linux directory to be a network location, and adds the file to provide security information for the copied file.\nThe Zone.Identifier file is used by Windows to help protect against the execution of potentially harmful files that may have been downloaded from the internet or copied from an untrusted source. When a file is opened, Windows checks the Zone.Identifier file to determine if the file is safe to execute.\nThis may prevent you from installing the file you just downloaded. In this case, simply delete the Zone.Identifier file.\n\n\n\nMake the script executable:\n\nchmod +x your_conda_installer.sh\n\nAnd run the bash script:\n\n./your_conda_installer.sh\n\nFollow the prompts, say yes to all. Quit WSL and start again so it recognizes the path to conda.\nYou’re done! Check your conda version using conda --version."
  },
  {
    "objectID": "tidbits/003_WSL/WSL.html#install-wsl",
    "href": "tidbits/003_WSL/WSL.html#install-wsl",
    "title": "Setting up a minimal data-science environment in Windows using WSL (part I)",
    "section": "",
    "text": "Install WSL: Open a powershell as administrator and run\n\nwsl --install\n\nCheck the installation and version with\n\nwsl -l -v\n\nTo activate the Bash shell type\n\nwsl\n\nTo exit the shell type\n\nexit\n\nYou can now use Bash to run linux tools and applications. During the installation process, make note of your login and password.\n\n\nYour home directory is accessed using cd ~. Create a Downloads directory and navigate to it:\n\ncd ~\nmkdir Downloads\ncd Downloads\n\nYou can use explorer.exe . to open File Explorer from the current directory."
  },
  {
    "objectID": "tidbits/003_WSL/WSL.html#update-the-linux-distribution",
    "href": "tidbits/003_WSL/WSL.html#update-the-linux-distribution",
    "title": "Setting up a minimal data-science environment in Windows using WSL (part I)",
    "section": "",
    "text": "Run the following code to update the linux distribution you just installed and make sur it is up-to-date:\n\nsudo apt update\nsudo apt upgrade"
  },
  {
    "objectID": "tidbits/003_WSL/WSL.html#install-python-3",
    "href": "tidbits/003_WSL/WSL.html#install-python-3",
    "title": "Setting up a minimal data-science environment in Windows using WSL (part I)",
    "section": "",
    "text": "Python comes pre-installed in WSL. Check the version:\n\npython3 --version\n\nIf somehow it is not installed, you can install it by running\n\nsudo apt install python3"
  },
  {
    "objectID": "tidbits/003_WSL/WSL.html#install-miniconda",
    "href": "tidbits/003_WSL/WSL.html#install-miniconda",
    "title": "Setting up a minimal data-science environment in Windows using WSL (part I)",
    "section": "",
    "text": "Even though WSL comes with python 3 pre-installed, we will use miniconda’s python distribution, which includes a package manager.\nGo to https://docs.conda.io/en/latest/miniconda.html to download the installer of your choice (listed under Linux). At the time of this writing the latest installer includes Python 3.10, and put it inside your ~/Downloads folder, which is accessible from windows using the above-mentioned explorer.exe . command.\nPlace your downloaded file in this directory.\n\n\n\n\n\n\nCopying files from Windows to WSL\n\n\n\n\n\nWhen you copy a file from Windows to a Linux directory in WSL, Windows may add a Zone.Identifier file to the directory containing the copied file. This is because Windows considers the Linux directory to be a network location, and adds the file to provide security information for the copied file.\nThe Zone.Identifier file is used by Windows to help protect against the execution of potentially harmful files that may have been downloaded from the internet or copied from an untrusted source. When a file is opened, Windows checks the Zone.Identifier file to determine if the file is safe to execute.\nThis may prevent you from installing the file you just downloaded. In this case, simply delete the Zone.Identifier file.\n\n\n\nMake the script executable:\n\nchmod +x your_conda_installer.sh\n\nAnd run the bash script:\n\n./your_conda_installer.sh\n\nFollow the prompts, say yes to all. Quit WSL and start again so it recognizes the path to conda.\nYou’re done! Check your conda version using conda --version."
  },
  {
    "objectID": "tidbits.html",
    "href": "tidbits.html",
    "title": "Tidbits",
    "section": "",
    "text": "I use this page to post small code snippets, helpers, howtos or hacks.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nConnecting to Strava using oanth2\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nRNN state space models\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSetting up a minimal data-science environment in Windows using WSL (part II)\n\n\n\n\n\n\n\ncode\n\n\nlinux\n\n\nsetup\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSetting up a minimal data-science environment in Windows using WSL (part I)\n\n\n\n\n\n\n\ncode\n\n\nlinux\n\n\nsetup\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/005_GLLVM-longi/longtinundinal_gaussian_nuisance.html#model-specification",
    "href": "posts/005_GLLVM-longi/longtinundinal_gaussian_nuisance.html#model-specification",
    "title": "GLLVM for longitudinal data",
    "section": "",
    "text": "Let \\(y_{i1t},y_{i2t},\\ldots,y_{ipt}\\) be a set of \\(p\\) response or observed variables at time \\(t,\\ t=1,\\ldots,T\\) for individual \\(i,\\ i=1,\\ldots,n\\). Let \\(\\mathbf{x}_{it}\\) be a set of observed \\(k\\)-dimensional covariates at time \\(t,\\ t=1,\\ldots,T\\).\nModels for multivariate longitudinal data have to account for the three sources of variability present in the data, that is (i) cross-sectional associations between the responses at a particular time point, (ii) cross-lagged associations between different responses at different occasions, and (iii) the association between repeated measures of the same response over time. The first source of variability is accounted for a time-dependent latent variable \\(z_{i1}, z_{i2},\\ldots,z_{iT}\\). Modeling the temporal evolution of the latent variable accounts for the cross-lagged associations between different responses over time. The third source of variability can be accounted for a set of item-specific random effects \\(\\mathbf{u}_{i}=(u_{i1}, \\ldots, u_{ip})'\\).\nAccording to the GLLVM framework we have\n\\[\\begin{align*}\n   \\nonumber y_{ijt}|\\mu_{ijt} &\\sim \\mathcal{F}_j(y_{ijt}\\vert \\mu_{ijt}, \\tau_j)\\\\\n   \\mu_{ijt}&=  g_j(\\eta_{ijt})=g_j(\\beta_{0jt} + \\mathbf{x}_{i}^{\\top}\\boldsymbol \\beta_{jt} + z_{it}^{\\top}\\lambda_{jt}+u_{ij}\\sigma_{u_j})\\\\ %  \\label{eqn:GLLVM-model2}\n\\end{align*}\\] where \\(g_j(\\cdot),j=1,\\ldots,p\\) is a known {}, \\(\\eta_{ijt}=\\beta_{0jt} + \\mathbf{x}_{i}^{\\top}\\boldsymbol \\beta_{jt} + z_{it}^{\\top}\\lambda_{jt}+u_{ij},i=1,\\ldots,n,j=1,\\ldots,p, t=1,\\ldots,T\\) is the {}, and \\(\\mathcal{F}_j(y_{ijt}\\vert \\eta_{ijt}, \\tau_j)\\) denotes a distribution from the exponential family with mean \\(\\mu_{ijt}\\) and response variable-specific dispersion parameter \\(\\tau_j\\). \\ The dynamics of the latent variable over time is modelled through a non-stationary autoregressive model of first order\n\\[\\begin{equation*}\nz_{it}=\\phi z_{i,t-1} + \\delta_{it}\n\\end{equation*}\\] where \\(z_{i1}\\sim N(0,\\sigma^2_{1})\\) and \\(\\delta_{it}\\sim N(0,1)\\). Moreover, we assume the random effects independent of the latent variable and their common distribution \\(\\mathbf{u}_{i}\\sim N_p(\\mathbf{0}, \\boldsymbol I)\\)."
  }
]