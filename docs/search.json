[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "test"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website.",
    "section": "",
    "text": "Under construction.\nThis is my website."
  },
  {
    "objectID": "latent-book.html",
    "href": "latent-book.html",
    "title": "Unobservable",
    "section": "",
    "text": "Unobservable: latent variable models for multivariate data. Freely accessible here."
  },
  {
    "objectID": "posts/001_vae/vae.html",
    "href": "posts/001_vae/vae.html",
    "title": "VAE Implementation for Nonlinear Latent Variable Modeling",
    "section": "",
    "text": "This is variational auto-encoder (VAE) implementation using tensorflow of the following non-linear latent variable model with one latent variable:\n\\[\\begin{align*}\n    Z &\\sim N(0, 1)\\\\\n    X_j | Z &= g_j(Z) + \\epsilon_j\\\\\n    \\epsilon_j &\\sim N(0, \\sigma_j^2), \\quad j=1, \\dots, p\n\\end{align*}\\]\nThis model can be likened to a non-linar factor analysis model, or, with the assumption that \\(\\sigma_j = \\sigma \\quad \\forall j\\), to a one-dimensional non-linear probabilistic principal component analysis (PPCA) model.\nGiven some observed data \\(\\{X^{(i)}\\}_{i=1}^n\\) with \\(X^{(i)} \\in \\mathbb R^p\\), the goal is to estimate the functions \\(g_j: \\mathbb R \\to \\mathbb R\\).\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers"
  },
  {
    "objectID": "posts/001_vae/vae.html#load-data",
    "href": "posts/001_vae/vae.html#load-data",
    "title": "VAE Implementation for Nonlinear Latent Variable Modeling",
    "section": "Load data",
    "text": "Load data\nWe load a toy dataset with dimensions \\(n=100, p=100\\).\n\ndata = pd.read_csv(\"./data/q1/data_1_200_100_1.csv\")\ndata = np.array(data, dtype='float32')\ndata = np.expand_dims(data, axis=-1)\ndata_train = data[:int(data.shape[0]*.9)]\ndata_test  = data[int(data.shape[0]*.9):]\n\n\ndata_train.shape\n\n(180, 100, 1)"
  },
  {
    "objectID": "posts/001_vae/vae.html#create-the-sampling-layer",
    "href": "posts/001_vae/vae.html#create-the-sampling-layer",
    "title": "VAE Implementation for Nonlinear Latent Variable Modeling",
    "section": "Create the sampling layer",
    "text": "Create the sampling layer\n\nclass Sampling(layers.Layer):\n    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
  },
  {
    "objectID": "posts/001_vae/vae.html#create-the-encoder",
    "href": "posts/001_vae/vae.html#create-the-encoder",
    "title": "VAE Implementation for Nonlinear Latent Variable Modeling",
    "section": "Create the encoder",
    "text": "Create the encoder\nThe encoder encodes the observed variables into the parameters of the posterior distribution of \\(Z^{(i)}|Y^{(i)}\\). Both the mean and variance share the first layer.\n\nlatent_dim = 1\n\nencoder_inputs = keras.Input(shape=data_train.shape[1:])\nx = layers.Flatten()(encoder_inputs)\nx = layers.Dense(100, activation=\"relu\")(x)\nz_mean = layers.Dense(100, activation=\"relu\")(x)\nz_mean = layers.Dense(100, activation=\"relu\")(x)\nz_mean =layers.Dense(latent_dim, name=\"z_mean\", activation=\"linear\")(z_mean)\n\nz_log_var = layers.Dense(100, activation=\"relu\")(x)\nz_log_var = layers.Dense(100, activation=\"relu\")(x)\nz_log_var = layers.Dense(latent_dim, name=\"z_log_var\", activation=\"linear\")(z_log_var)\nz = Sampling()([z_mean, z_log_var])\n\nencoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n\nencoder.summary()\n\nModel: \"encoder\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 100, 1)]     0           []                               \n                                                                                                  \n flatten (Flatten)              (None, 100)          0           ['input_1[0][0]']                \n                                                                                                  \n dense (Dense)                  (None, 100)          10100       ['flatten[0][0]']                \n                                                                                                  \n dense_2 (Dense)                (None, 100)          10100       ['dense[0][0]']                  \n                                                                                                  \n dense_4 (Dense)                (None, 100)          10100       ['dense[0][0]']                  \n                                                                                                  \n z_mean (Dense)                 (None, 1)            101         ['dense_2[0][0]']                \n                                                                                                  \n z_log_var (Dense)              (None, 1)            101         ['dense_4[0][0]']                \n                                                                                                  \n sampling (Sampling)            (None, 1)            0           ['z_mean[0][0]',                 \n                                                                  'z_log_var[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 30,502\nTrainable params: 30,502\nNon-trainable params: 0\n__________________________________________________________________________________________________"
  },
  {
    "objectID": "posts/001_vae/vae.html#create-the-decoder",
    "href": "posts/001_vae/vae.html#create-the-decoder",
    "title": "VAE Implementation for Nonlinear Latent Variable Modeling",
    "section": "Create the Decoder",
    "text": "Create the Decoder\nThe decoder has to be flexible enough to be able to model the functions \\(g_j\\), which models the conditional means of the responses. On top of that, the decoder models the residual variances \\(\\sigma_j\\).\n\nlatent_inputs = keras.Input(shape=(latent_dim,))\nx = layers.Dense(50, activation=\"relu\")(latent_inputs)\nx = layers.Dense(100, activation=\"relu\")(x)\n\nlog_var = layers.Dense(100, activation=\"relu\")(x)\nlog_var = layers.Dense(100, activation=\"linear\")(log_var)\nlog_var = layers.Reshape(data_train.shape[1:])(log_var)\n\n\nx_output = layers.Dense(100, activation=\"relu\")(x)\nx_output = layers.Dense(100, activation=\"linear\")(x_output)\nx_output = layers.Reshape(data_train.shape[1:])(x_output)\n\ndecoder = keras.Model(latent_inputs, [x_output, log_var], name=\"decoder\")\ndecoder.summary()\n\nModel: \"decoder\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 1)]          0           []                               \n                                                                                                  \n dense_5 (Dense)                (None, 50)           100         ['input_2[0][0]']                \n                                                                                                  \n dense_6 (Dense)                (None, 100)          5100        ['dense_5[0][0]']                \n                                                                                                  \n dense_9 (Dense)                (None, 100)          10100       ['dense_6[0][0]']                \n                                                                                                  \n dense_7 (Dense)                (None, 100)          10100       ['dense_6[0][0]']                \n                                                                                                  \n dense_10 (Dense)               (None, 100)          10100       ['dense_9[0][0]']                \n                                                                                                  \n dense_8 (Dense)                (None, 100)          10100       ['dense_7[0][0]']                \n                                                                                                  \n reshape_1 (Reshape)            (None, 100, 1)       0           ['dense_10[0][0]']               \n                                                                                                  \n reshape (Reshape)              (None, 100, 1)       0           ['dense_8[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 45,600\nTrainable params: 45,600\nNon-trainable params: 0\n__________________________________________________________________________________________________"
  },
  {
    "objectID": "posts/001_vae/vae.html#create-the-variational-autoencoder",
    "href": "posts/001_vae/vae.html#create-the-variational-autoencoder",
    "title": "VAE Implementation for Nonlinear Latent Variable Modeling",
    "section": "Create the variational autoencoder",
    "text": "Create the variational autoencoder\nThe variational auto-encoder (VAE) is itself a keras.Model object. It consists of the encoder and decoder layers, a specialized train_step as well of some specialized metrics to keep track of our progress.\n\nclass VAE(keras.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super().__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = keras.metrics.Mean(\n            name=\"reconstruction_loss\"\n        )\n        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, z = self.encoder(data)\n            reconstruction, logvar = self.decoder(z)\n            logvar = tf.reduce_mean(logvar, axis=1, keepdims=True)\n\n            # Reconstruction loss (including the residual variances)\n            reconstruction_loss = 0.5 * tf.reduce_mean(tf.reduce_sum((data - reconstruction)**2 * tf.math.exp(-logvar) + logvar + tf.math.log(2. * np.pi), axis=1))\n\n            # ELBO loss, approximating the posterior with a Gaussian.\n            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n            total_loss = reconstruction_loss + kl_loss\n\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n        return {\n            \"total_loss\": self.total_loss_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n            \"kl_loss\": self.kl_loss_tracker.result(),\n        }\n\nNow Markdown\n\nvae = VAE(encoder, decoder)\nvae.compile(optimizer=keras.optimizers.Adam())\nvae.fit(data_train, epochs=200)"
  },
  {
    "objectID": "posts/001_vae/vae.html#plot",
    "href": "posts/001_vae/vae.html#plot",
    "title": "VAE Implementation for Nonlinear Latent Variable Modeling",
    "section": "Plot",
    "text": "Plot\nWe now plot the fit, i.e. the reconstruction vs the original data.\n\nencoded, _, _ = vae.encoder.predict(data_train)\ndecoded, var = vae.decoder.predict(encoded)\n\nf, ax = plt.subplots()\nax.scatter(\n    data_train.reshape((np.prod(decoded.shape),)),\n    decoded.reshape((np.prod(decoded.shape),))\n)\nax.plot([-5,5], [-5,5], 'k-')\nplt.title(\"Reconstructed vs Original data.\")\nplt.xlabel(\"Original data.\")\nplt.ylabel(\"Reconstructed data.\")\nplt.show()\n\n6/6 [==============================] - 0s 3ms/step\n6/6 [==============================] - 0s 2ms/step\n\n\n\n\n\nWe can also plot the estimated latent variables (the encoded data), which should approximately be standard normal random variables.\n\nplt.hist(encoded)\nplt.title(\"Empirical distribution of the estimated latent variables.\")\nplt.xlabel(\"Value of the estimated latent variable.\")\nplt.ylabel(\"Frequency\")\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\nFinally, we can also plot a selection of the \\(g_j\\) functions.\n\n# Define a range of values to plot\nz_grid = np.arange(-3,3, .1)\nz_grid = np.expand_dims(z_grid, axis=1)\n\n# Obtain the map of these values through the estimated g_j functions\ny_grid, _ = vae.decoder.predict(z_grid)\n\n# Plot 20 of these\nn = 20\nplt.figure()\nfor i in range(n):\n    plt.subplot(4,int(n/4),i+1)\n    plt.plot(z_grid[:,-1], y_grid[:,i, -1])\n\n2/2 [==============================] - 0s 5ms/step"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Iris K-Means Clustering\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nVAE Implementation for Nonlinear Latent Variable Modeling\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\nGuillaum eBlanc\n\n\n\n\n\n\nNo matching items"
  }
]