[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nVAE implementation for Nonlinar Latent Variable Modeling\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nGuillaume Blanc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2023\n\n\nTristan Oâ€™Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website.",
    "section": "",
    "text": "Under construction.\nThis is my website."
  },
  {
    "objectID": "latent-book.html",
    "href": "latent-book.html",
    "title": "Unobservable",
    "section": "",
    "text": "Unobservable: latent variable models for multivariate data. Freely accessible here."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/vae/vae.html",
    "href": "posts/vae/vae.html",
    "title": "VAE implementation for Nonlinar Latent Variable Modeling",
    "section": "",
    "text": "This is variational auto-encoder (VAE) implementation using tensorflow of the following non-linear latent variable model with one latent variable:\n\\[\nZ \\sim N(0, 1)\\\\\nX_j | Z = g_j(Z) + \\epsilon_j\\\\\n\\epsilon_j \\sim N(0, \\sigma_j^2), \\quad j=1, \\dots, p\n\\]\nThis model can be likened to a non-linar factor analysis model, or, with the assumption that \\(\\sigma_j = \\sigma \\quad \\forall j\\), to a one-dimensional non-linear probabilistic principal component analysis (PPCA) model.\nGiven some observed data \\(\\{X^{(i)}\\}_{i=1}^n\\) with \\(X^{(i)} \\in \\mathbb R^p\\), the goal is to estimate the functions \\(g_j: \\mathbb R \\to \\mathbb R\\).\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\n\nWe load a toy dataset with dimensions \\(n=100, p=100\\).\n\ndata = pd.read_csv(\"./data/q1/data_1_200_100_1.csv\")\ndata = np.array(data, dtype='float32')\ndata = np.expand_dims(data, axis=-1)\ndata_train = data[:int(data.shape[0]*.9)]\ndata_test  = data[int(data.shape[0]*.9):]\n\n\ndata_train.shape\n\n(180, 100, 1)\n\n\n\n\n\n\nclass Sampling(layers.Layer):\n    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\n\n\n\nThe encoder encodes the observed variables into the parameters of the posterior distribution of \\(Z^{(i)}|Y^{(i)}\\). Both the mean and variance share the first layer.\n\nlatent_dim = 1\n\nencoder_inputs = keras.Input(shape=data_train.shape[1:])\nx = layers.Flatten()(encoder_inputs)\nx = layers.Dense(100, activation=\"relu\")(x)\nz_mean = layers.Dense(100, activation=\"relu\")(x)\nz_mean = layers.Dense(100, activation=\"relu\")(x)\nz_mean =layers.Dense(latent_dim, name=\"z_mean\", activation=\"linear\")(z_mean)\n\nz_log_var = layers.Dense(100, activation=\"relu\")(x)\nz_log_var = layers.Dense(100, activation=\"relu\")(x)\nz_log_var = layers.Dense(latent_dim, name=\"z_log_var\", activation=\"linear\")(z_log_var)\nz = Sampling()([z_mean, z_log_var])\n\nencoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n\nencoder.summary()\n\nModel: \"encoder\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 100, 1)]     0           []                               \n                                                                                                  \n flatten (Flatten)              (None, 100)          0           ['input_1[0][0]']                \n                                                                                                  \n dense (Dense)                  (None, 100)          10100       ['flatten[0][0]']                \n                                                                                                  \n dense_2 (Dense)                (None, 100)          10100       ['dense[0][0]']                  \n                                                                                                  \n dense_4 (Dense)                (None, 100)          10100       ['dense[0][0]']                  \n                                                                                                  \n z_mean (Dense)                 (None, 1)            101         ['dense_2[0][0]']                \n                                                                                                  \n z_log_var (Dense)              (None, 1)            101         ['dense_4[0][0]']                \n                                                                                                  \n sampling (Sampling)            (None, 1)            0           ['z_mean[0][0]',                 \n                                                                  'z_log_var[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 30,502\nTrainable params: 30,502\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\n\n\n\nThe decoder has to be flexible enough to be able to model the functions \\(g_j\\), which models the conditional means of the responses. On top of that, the decoder models the residual variances \\(\\sigma_j\\).\n\nlatent_inputs = keras.Input(shape=(latent_dim,))\nx = layers.Dense(50, activation=\"relu\")(latent_inputs)\nx = layers.Dense(100, activation=\"relu\")(x)\n\nlog_var = layers.Dense(100, activation=\"relu\")(x)\nlog_var = layers.Dense(100, activation=\"linear\")(log_var)\nlog_var = layers.Reshape(data_train.shape[1:])(log_var)\n\n\nx_output = layers.Dense(100, activation=\"relu\")(x)\nx_output = layers.Dense(100, activation=\"linear\")(x_output)\nx_output = layers.Reshape(data_train.shape[1:])(x_output)\n\ndecoder = keras.Model(latent_inputs, [x_output, log_var], name=\"decoder\")\ndecoder.summary()\n\nModel: \"decoder\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 1)]          0           []                               \n                                                                                                  \n dense_5 (Dense)                (None, 50)           100         ['input_2[0][0]']                \n                                                                                                  \n dense_6 (Dense)                (None, 100)          5100        ['dense_5[0][0]']                \n                                                                                                  \n dense_9 (Dense)                (None, 100)          10100       ['dense_6[0][0]']                \n                                                                                                  \n dense_7 (Dense)                (None, 100)          10100       ['dense_6[0][0]']                \n                                                                                                  \n dense_10 (Dense)               (None, 100)          10100       ['dense_9[0][0]']                \n                                                                                                  \n dense_8 (Dense)                (None, 100)          10100       ['dense_7[0][0]']                \n                                                                                                  \n reshape_1 (Reshape)            (None, 100, 1)       0           ['dense_10[0][0]']               \n                                                                                                  \n reshape (Reshape)              (None, 100, 1)       0           ['dense_8[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 45,600\nTrainable params: 45,600\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\n\n\n\nThe variational auto-encoder (VAE) is itself a keras.Model object. It consists of the encoder and decoder layers, a specialized train_step as well of some specialized metrics to keep track of our progress.\n\nclass VAE(keras.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super().__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = keras.metrics.Mean(\n            name=\"reconstruction_loss\"\n        )\n        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, z = self.encoder(data)\n            reconstruction, logvar = self.decoder(z)\n            logvar = tf.reduce_mean(logvar, axis=1, keepdims=True)\n\n            # Reconstruction loss (including the residual variances)\n            reconstruction_loss = 0.5 * tf.reduce_mean(tf.reduce_sum((data - reconstruction)**2 * tf.math.exp(-logvar) + logvar + tf.math.log(2. * np.pi), axis=1))\n\n            # ELBO loss, approximating the posterior with a Gaussian.\n            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n            total_loss = reconstruction_loss + kl_loss\n\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n        return {\n            \"total_loss\": self.total_loss_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n            \"kl_loss\": self.kl_loss_tracker.result(),\n        }\n\n\nvae = VAE(encoder, decoder)\nvae.compile(optimizer=keras.optimizers.Adam())\nvae.fit(data_train, epochs=200)\n\nEpoch 1/200\n6/6 [==============================] - 5s 5ms/step - total_loss: 159.4694 - reconstruction_loss: 158.8095 - kl_loss: 0.6599\nEpoch 2/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 157.3188 - reconstruction_loss: 156.3394 - kl_loss: 0.9794\nEpoch 3/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 154.5943 - reconstruction_loss: 153.0545 - kl_loss: 1.5398\nEpoch 4/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 152.1033 - reconstruction_loss: 150.5485 - kl_loss: 1.5548\nEpoch 5/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 147.7574 - reconstruction_loss: 144.8417 - kl_loss: 2.9157\nEpoch 6/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 142.2038 - reconstruction_loss: 138.8096 - kl_loss: 3.3943\nEpoch 7/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 136.3394 - reconstruction_loss: 133.4327 - kl_loss: 2.9067\nEpoch 8/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 132.0306 - reconstruction_loss: 129.1889 - kl_loss: 2.8417\nEpoch 9/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 128.4884 - reconstruction_loss: 125.7331 - kl_loss: 2.7553\nEpoch 10/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 126.0616 - reconstruction_loss: 123.3086 - kl_loss: 2.7530\nEpoch 11/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 123.9195 - reconstruction_loss: 121.1649 - kl_loss: 2.7547\nEpoch 12/200\n6/6 [==============================] - 0s 5ms/step - total_loss: 123.4685 - reconstruction_loss: 120.7576 - kl_loss: 2.7109\nEpoch 13/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 121.9943 - reconstruction_loss: 119.0857 - kl_loss: 2.9086\nEpoch 14/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 121.4584 - reconstruction_loss: 118.4306 - kl_loss: 3.0278\nEpoch 15/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 121.2427 - reconstruction_loss: 118.3778 - kl_loss: 2.8650\nEpoch 16/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 120.2809 - reconstruction_loss: 117.4173 - kl_loss: 2.8635\nEpoch 17/200\n6/6 [==============================] - 0s 5ms/step - total_loss: 119.4762 - reconstruction_loss: 116.6165 - kl_loss: 2.8597\nEpoch 18/200\n6/6 [==============================] - 0s 5ms/step - total_loss: 118.1459 - reconstruction_loss: 115.3286 - kl_loss: 2.8172\nEpoch 19/200\n6/6 [==============================] - 0s 13ms/step - total_loss: 117.4240 - reconstruction_loss: 114.6200 - kl_loss: 2.8040\nEpoch 20/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 117.1775 - reconstruction_loss: 114.3382 - kl_loss: 2.8392\nEpoch 21/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 116.7512 - reconstruction_loss: 113.8932 - kl_loss: 2.8580\nEpoch 22/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 114.6956 - reconstruction_loss: 111.8335 - kl_loss: 2.8621\nEpoch 23/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 114.0700 - reconstruction_loss: 111.2253 - kl_loss: 2.8447\nEpoch 24/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 111.6568 - reconstruction_loss: 108.7404 - kl_loss: 2.9164\nEpoch 25/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 110.1693 - reconstruction_loss: 107.2955 - kl_loss: 2.8737\nEpoch 26/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 108.0621 - reconstruction_loss: 105.1189 - kl_loss: 2.9431\nEpoch 27/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 105.9520 - reconstruction_loss: 102.7703 - kl_loss: 3.1818\nEpoch 28/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 104.2761 - reconstruction_loss: 101.0551 - kl_loss: 3.2210\nEpoch 29/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 102.5817 - reconstruction_loss: 99.3796 - kl_loss: 3.2020\nEpoch 30/200\n6/6 [==============================] - 0s 18ms/step - total_loss: 100.3066 - reconstruction_loss: 97.1519 - kl_loss: 3.1548\nEpoch 31/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 99.7117 - reconstruction_loss: 96.4821 - kl_loss: 3.2297\nEpoch 32/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 97.7306 - reconstruction_loss: 94.2878 - kl_loss: 3.4428\nEpoch 33/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 96.3645 - reconstruction_loss: 92.9167 - kl_loss: 3.4479\nEpoch 34/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 95.0677 - reconstruction_loss: 91.7078 - kl_loss: 3.3598\nEpoch 35/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 94.1247 - reconstruction_loss: 90.6508 - kl_loss: 3.4740\nEpoch 36/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 94.0376 - reconstruction_loss: 90.4024 - kl_loss: 3.6352\nEpoch 37/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 92.4411 - reconstruction_loss: 88.6930 - kl_loss: 3.7481\nEpoch 38/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 91.0048 - reconstruction_loss: 87.3415 - kl_loss: 3.6633\nEpoch 39/200\n6/6 [==============================] - 0s 30ms/step - total_loss: 89.7175 - reconstruction_loss: 86.2138 - kl_loss: 3.5037\nEpoch 40/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 89.1252 - reconstruction_loss: 85.4268 - kl_loss: 3.6984\nEpoch 41/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 87.8362 - reconstruction_loss: 83.8817 - kl_loss: 3.9545\nEpoch 42/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 87.6689 - reconstruction_loss: 83.7613 - kl_loss: 3.9076\nEpoch 43/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 87.3709 - reconstruction_loss: 83.5801 - kl_loss: 3.7908\nEpoch 44/200\n6/6 [==============================] - 0s 17ms/step - total_loss: 85.8306 - reconstruction_loss: 81.8511 - kl_loss: 3.9795\nEpoch 45/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 85.7300 - reconstruction_loss: 81.7236 - kl_loss: 4.0064\nEpoch 46/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 85.5675 - reconstruction_loss: 81.6855 - kl_loss: 3.8820\nEpoch 47/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 85.3483 - reconstruction_loss: 81.4715 - kl_loss: 3.8768\nEpoch 48/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 85.4788 - reconstruction_loss: 81.6757 - kl_loss: 3.8031\nEpoch 49/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 84.3944 - reconstruction_loss: 80.5271 - kl_loss: 3.8674\nEpoch 50/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 83.8853 - reconstruction_loss: 79.8080 - kl_loss: 4.0773\nEpoch 51/200\n6/6 [==============================] - 0s 14ms/step - total_loss: 83.1672 - reconstruction_loss: 79.2457 - kl_loss: 3.9215\nEpoch 52/200\n6/6 [==============================] - 0s 20ms/step - total_loss: 83.2434 - reconstruction_loss: 79.4520 - kl_loss: 3.7914\nEpoch 53/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 83.0088 - reconstruction_loss: 79.0819 - kl_loss: 3.9269\nEpoch 54/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 81.8423 - reconstruction_loss: 77.8640 - kl_loss: 3.9783\nEpoch 55/200\n6/6 [==============================] - 0s 14ms/step - total_loss: 81.7239 - reconstruction_loss: 77.8502 - kl_loss: 3.8737\nEpoch 56/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 81.2914 - reconstruction_loss: 77.5255 - kl_loss: 3.7659\nEpoch 57/200\n6/6 [==============================] - 0s 24ms/step - total_loss: 81.2742 - reconstruction_loss: 77.5173 - kl_loss: 3.7568\nEpoch 58/200\n6/6 [==============================] - 0s 16ms/step - total_loss: 81.4382 - reconstruction_loss: 77.6066 - kl_loss: 3.8316\nEpoch 59/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 81.4028 - reconstruction_loss: 77.5080 - kl_loss: 3.8948\nEpoch 60/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 81.1439 - reconstruction_loss: 77.1630 - kl_loss: 3.9810\nEpoch 61/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 80.4591 - reconstruction_loss: 76.5975 - kl_loss: 3.8615\nEpoch 62/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 80.4920 - reconstruction_loss: 76.7512 - kl_loss: 3.7409\nEpoch 63/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 80.4919 - reconstruction_loss: 76.6208 - kl_loss: 3.8711\nEpoch 64/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 80.5306 - reconstruction_loss: 76.4850 - kl_loss: 4.0457\nEpoch 65/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 80.0165 - reconstruction_loss: 76.0480 - kl_loss: 3.9685\nEpoch 66/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 80.0417 - reconstruction_loss: 76.3429 - kl_loss: 3.6988\nEpoch 67/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 79.9329 - reconstruction_loss: 76.2699 - kl_loss: 3.6630\nEpoch 68/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 79.9716 - reconstruction_loss: 76.1393 - kl_loss: 3.8324\nEpoch 69/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 80.6046 - reconstruction_loss: 76.6400 - kl_loss: 3.9647\nEpoch 70/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 79.8648 - reconstruction_loss: 75.8009 - kl_loss: 4.0638\nEpoch 71/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 79.7548 - reconstruction_loss: 75.7707 - kl_loss: 3.9841\nEpoch 72/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 79.5776 - reconstruction_loss: 75.6953 - kl_loss: 3.8823\nEpoch 73/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 79.5520 - reconstruction_loss: 75.6548 - kl_loss: 3.8971\nEpoch 74/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 79.5209 - reconstruction_loss: 75.5917 - kl_loss: 3.9292\nEpoch 75/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 79.3321 - reconstruction_loss: 75.3729 - kl_loss: 3.9591\nEpoch 76/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 78.8106 - reconstruction_loss: 74.7718 - kl_loss: 4.0388\nEpoch 77/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 78.3486 - reconstruction_loss: 74.4723 - kl_loss: 3.8763\nEpoch 78/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 78.4589 - reconstruction_loss: 74.6674 - kl_loss: 3.7914\nEpoch 79/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 78.1411 - reconstruction_loss: 74.1538 - kl_loss: 3.9873\nEpoch 80/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 77.9273 - reconstruction_loss: 73.9364 - kl_loss: 3.9909\nEpoch 81/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 77.6897 - reconstruction_loss: 73.8694 - kl_loss: 3.8203\nEpoch 82/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 77.6744 - reconstruction_loss: 73.9221 - kl_loss: 3.7522\nEpoch 83/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 77.5535 - reconstruction_loss: 73.8011 - kl_loss: 3.7523\nEpoch 84/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 77.9563 - reconstruction_loss: 74.0288 - kl_loss: 3.9275\nEpoch 85/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 77.7252 - reconstruction_loss: 73.7183 - kl_loss: 4.0069\nEpoch 86/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 77.2565 - reconstruction_loss: 73.4258 - kl_loss: 3.8307\nEpoch 87/200\n6/6 [==============================] - 0s 18ms/step - total_loss: 78.0296 - reconstruction_loss: 74.2869 - kl_loss: 3.7428\nEpoch 88/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 77.7511 - reconstruction_loss: 73.7531 - kl_loss: 3.9980\nEpoch 89/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 77.2437 - reconstruction_loss: 73.1683 - kl_loss: 4.0753\nEpoch 90/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 77.6239 - reconstruction_loss: 73.6363 - kl_loss: 3.9876\nEpoch 91/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 76.9329 - reconstruction_loss: 73.0359 - kl_loss: 3.8971\nEpoch 92/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 77.0101 - reconstruction_loss: 73.1212 - kl_loss: 3.8888\nEpoch 93/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 76.9049 - reconstruction_loss: 72.9713 - kl_loss: 3.9337\nEpoch 94/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 76.8861 - reconstruction_loss: 72.9856 - kl_loss: 3.9005\nEpoch 95/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 76.7297 - reconstruction_loss: 72.8783 - kl_loss: 3.8513\nEpoch 96/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 76.8668 - reconstruction_loss: 73.0576 - kl_loss: 3.8092\nEpoch 97/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 77.0898 - reconstruction_loss: 73.2349 - kl_loss: 3.8549\nEpoch 98/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 76.6132 - reconstruction_loss: 72.7206 - kl_loss: 3.8926\nEpoch 99/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 77.0141 - reconstruction_loss: 73.0733 - kl_loss: 3.9408\nEpoch 100/200\n6/6 [==============================] - 0s 13ms/step - total_loss: 76.4530 - reconstruction_loss: 72.4564 - kl_loss: 3.9967\nEpoch 101/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 76.6080 - reconstruction_loss: 72.6005 - kl_loss: 4.0075\nEpoch 102/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 76.6930 - reconstruction_loss: 72.6261 - kl_loss: 4.0669\nEpoch 103/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 76.7384 - reconstruction_loss: 72.8002 - kl_loss: 3.9383\nEpoch 104/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 76.4935 - reconstruction_loss: 72.6563 - kl_loss: 3.8372\nEpoch 105/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 76.3498 - reconstruction_loss: 72.5485 - kl_loss: 3.8012\nEpoch 106/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 76.2614 - reconstruction_loss: 72.3045 - kl_loss: 3.9570\nEpoch 107/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 76.1234 - reconstruction_loss: 72.1581 - kl_loss: 3.9653\nEpoch 108/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 76.1155 - reconstruction_loss: 72.2819 - kl_loss: 3.8336\nEpoch 109/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.7346 - reconstruction_loss: 71.9616 - kl_loss: 3.7731\nEpoch 110/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 76.0551 - reconstruction_loss: 72.2147 - kl_loss: 3.8403\nEpoch 111/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 76.6325 - reconstruction_loss: 72.6785 - kl_loss: 3.9540\nEpoch 112/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 76.2791 - reconstruction_loss: 72.3832 - kl_loss: 3.8959\nEpoch 113/200\n6/6 [==============================] - 0s 14ms/step - total_loss: 76.3594 - reconstruction_loss: 72.5248 - kl_loss: 3.8346\nEpoch 114/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.7003 - reconstruction_loss: 71.9479 - kl_loss: 3.7525\nEpoch 115/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 75.8326 - reconstruction_loss: 72.0700 - kl_loss: 3.7626\nEpoch 116/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.8259 - reconstruction_loss: 71.8656 - kl_loss: 3.9603\nEpoch 117/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.9852 - reconstruction_loss: 72.0045 - kl_loss: 3.9808\nEpoch 118/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.5266 - reconstruction_loss: 71.6431 - kl_loss: 3.8835\nEpoch 119/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 75.8531 - reconstruction_loss: 71.9800 - kl_loss: 3.8731\nEpoch 120/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.4883 - reconstruction_loss: 71.5299 - kl_loss: 3.9584\nEpoch 121/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 75.7388 - reconstruction_loss: 71.7915 - kl_loss: 3.9474\nEpoch 122/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.5227 - reconstruction_loss: 71.7053 - kl_loss: 3.8173\nEpoch 123/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.2283 - reconstruction_loss: 71.4103 - kl_loss: 3.8180\nEpoch 124/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.4891 - reconstruction_loss: 71.6811 - kl_loss: 3.8079\nEpoch 125/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.7789 - reconstruction_loss: 71.9703 - kl_loss: 3.8086\nEpoch 126/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.8799 - reconstruction_loss: 72.0370 - kl_loss: 3.8429\nEpoch 127/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.8861 - reconstruction_loss: 71.9968 - kl_loss: 3.8893\nEpoch 128/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.2229 - reconstruction_loss: 71.3336 - kl_loss: 3.8892\nEpoch 129/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.5182 - reconstruction_loss: 71.6709 - kl_loss: 3.8473\nEpoch 130/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.4523 - reconstruction_loss: 71.6015 - kl_loss: 3.8508\nEpoch 131/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 75.1373 - reconstruction_loss: 71.2577 - kl_loss: 3.8796\nEpoch 132/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.0030 - reconstruction_loss: 71.1554 - kl_loss: 3.8476\nEpoch 133/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 75.2704 - reconstruction_loss: 71.5029 - kl_loss: 3.7676\nEpoch 134/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.1937 - reconstruction_loss: 71.4221 - kl_loss: 3.7716\nEpoch 135/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.2062 - reconstruction_loss: 71.4695 - kl_loss: 3.7366\nEpoch 136/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 75.5514 - reconstruction_loss: 71.7997 - kl_loss: 3.7517\nEpoch 137/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 75.3288 - reconstruction_loss: 71.5681 - kl_loss: 3.7607\nEpoch 138/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.8560 - reconstruction_loss: 71.0130 - kl_loss: 3.8430\nEpoch 139/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.8883 - reconstruction_loss: 71.1001 - kl_loss: 3.7883\nEpoch 140/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 75.0647 - reconstruction_loss: 71.2895 - kl_loss: 3.7752\nEpoch 141/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 74.9836 - reconstruction_loss: 71.2304 - kl_loss: 3.7532\nEpoch 142/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.1171 - reconstruction_loss: 71.3837 - kl_loss: 3.7335\nEpoch 143/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 74.6719 - reconstruction_loss: 70.7620 - kl_loss: 3.9100\nEpoch 144/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 74.9929 - reconstruction_loss: 71.1038 - kl_loss: 3.8891\nEpoch 145/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 74.9700 - reconstruction_loss: 71.1671 - kl_loss: 3.8029\nEpoch 146/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.7118 - reconstruction_loss: 71.0037 - kl_loss: 3.7081\nEpoch 147/200\n6/6 [==============================] - 0s 13ms/step - total_loss: 74.7556 - reconstruction_loss: 70.9940 - kl_loss: 3.7616\nEpoch 148/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 75.2549 - reconstruction_loss: 71.4497 - kl_loss: 3.8052\nEpoch 149/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 75.3068 - reconstruction_loss: 71.4709 - kl_loss: 3.8358\nEpoch 150/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.5678 - reconstruction_loss: 71.8368 - kl_loss: 3.7310\nEpoch 151/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.6995 - reconstruction_loss: 71.8900 - kl_loss: 3.8095\nEpoch 152/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.5212 - reconstruction_loss: 71.6031 - kl_loss: 3.9180\nEpoch 153/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.4468 - reconstruction_loss: 71.5346 - kl_loss: 3.9122\nEpoch 154/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.0431 - reconstruction_loss: 71.1578 - kl_loss: 3.8854\nEpoch 155/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 74.8433 - reconstruction_loss: 71.0172 - kl_loss: 3.8261\nEpoch 156/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 75.0288 - reconstruction_loss: 71.2482 - kl_loss: 3.7805\nEpoch 157/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 74.8153 - reconstruction_loss: 70.9827 - kl_loss: 3.8326\nEpoch 158/200\n6/6 [==============================] - 0s 13ms/step - total_loss: 75.0699 - reconstruction_loss: 71.2169 - kl_loss: 3.8530\nEpoch 159/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 74.8352 - reconstruction_loss: 71.0610 - kl_loss: 3.7742\nEpoch 160/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.0455 - reconstruction_loss: 71.2989 - kl_loss: 3.7466\nEpoch 161/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 74.9925 - reconstruction_loss: 71.2544 - kl_loss: 3.7381\nEpoch 162/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 75.1559 - reconstruction_loss: 71.4062 - kl_loss: 3.7497\nEpoch 163/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 74.7373 - reconstruction_loss: 70.9925 - kl_loss: 3.7449\nEpoch 164/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 75.0982 - reconstruction_loss: 71.3413 - kl_loss: 3.7569\nEpoch 165/200\n6/6 [==============================] - 0s 14ms/step - total_loss: 74.7588 - reconstruction_loss: 70.9353 - kl_loss: 3.8235\nEpoch 166/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 75.2021 - reconstruction_loss: 71.3836 - kl_loss: 3.8186\nEpoch 167/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 74.8396 - reconstruction_loss: 70.9573 - kl_loss: 3.8823\nEpoch 168/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 74.9674 - reconstruction_loss: 71.1168 - kl_loss: 3.8506\nEpoch 169/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 75.1807 - reconstruction_loss: 71.3710 - kl_loss: 3.8097\nEpoch 170/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 74.8106 - reconstruction_loss: 71.0109 - kl_loss: 3.7997\nEpoch 171/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 75.7396 - reconstruction_loss: 71.8767 - kl_loss: 3.8629\nEpoch 172/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.0245 - reconstruction_loss: 71.1096 - kl_loss: 3.9150\nEpoch 173/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 74.6527 - reconstruction_loss: 70.8753 - kl_loss: 3.7774\nEpoch 174/200\n6/6 [==============================] - 0s 15ms/step - total_loss: 74.5188 - reconstruction_loss: 70.8158 - kl_loss: 3.7029\nEpoch 175/200\n6/6 [==============================] - 0s 13ms/step - total_loss: 74.2905 - reconstruction_loss: 70.5769 - kl_loss: 3.7136\nEpoch 176/200\n6/6 [==============================] - 0s 14ms/step - total_loss: 74.4287 - reconstruction_loss: 70.6487 - kl_loss: 3.7800\nEpoch 177/200\n6/6 [==============================] - 0s 14ms/step - total_loss: 74.2941 - reconstruction_loss: 70.5216 - kl_loss: 3.7725\nEpoch 178/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 74.5420 - reconstruction_loss: 70.7651 - kl_loss: 3.7769\nEpoch 179/200\n6/6 [==============================] - 0s 20ms/step - total_loss: 74.3038 - reconstruction_loss: 70.4219 - kl_loss: 3.8819\nEpoch 180/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 74.5688 - reconstruction_loss: 70.7080 - kl_loss: 3.8608\nEpoch 181/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 74.5755 - reconstruction_loss: 70.7965 - kl_loss: 3.7790\nEpoch 182/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 74.8235 - reconstruction_loss: 71.1014 - kl_loss: 3.7220\nEpoch 183/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.3601 - reconstruction_loss: 70.6418 - kl_loss: 3.7183\nEpoch 184/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.7422 - reconstruction_loss: 71.0048 - kl_loss: 3.7374\nEpoch 185/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 74.5962 - reconstruction_loss: 70.8452 - kl_loss: 3.7511\nEpoch 186/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 74.4384 - reconstruction_loss: 70.7097 - kl_loss: 3.7287\nEpoch 187/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.6677 - reconstruction_loss: 70.8648 - kl_loss: 3.8030\nEpoch 188/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.7798 - reconstruction_loss: 70.9518 - kl_loss: 3.8280\nEpoch 189/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.7135 - reconstruction_loss: 70.9471 - kl_loss: 3.7664\nEpoch 190/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 74.9372 - reconstruction_loss: 71.2350 - kl_loss: 3.7022\nEpoch 191/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.1655 - reconstruction_loss: 71.3957 - kl_loss: 3.7698\nEpoch 192/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.6117 - reconstruction_loss: 70.6775 - kl_loss: 3.9343\nEpoch 193/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.9547 - reconstruction_loss: 71.0987 - kl_loss: 3.8560\nEpoch 194/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.1745 - reconstruction_loss: 71.3602 - kl_loss: 3.8143\nEpoch 195/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 74.7096 - reconstruction_loss: 70.9043 - kl_loss: 3.8053\nEpoch 196/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 75.4985 - reconstruction_loss: 71.7083 - kl_loss: 3.7902\nEpoch 197/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 75.2293 - reconstruction_loss: 71.4085 - kl_loss: 3.8209\nEpoch 198/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 74.9497 - reconstruction_loss: 71.1077 - kl_loss: 3.8421\nEpoch 199/200\n6/6 [==============================] - 0s 14ms/step - total_loss: 74.9374 - reconstruction_loss: 71.1824 - kl_loss: 3.7549\nEpoch 200/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 74.5571 - reconstruction_loss: 70.8363 - kl_loss: 3.7208\n\n\n\n\n\nWe now plot the fit, i.e.Â the reconstruction vs the original data.\n\nencoded, _, _ = vae.encoder.predict(data_train)\ndecoded, var = vae.decoder.predict(encoded)\n\nf, ax = plt.subplots()\nax.scatter(\n    data_train.reshape((np.prod(decoded.shape),)),\n    decoded.reshape((np.prod(decoded.shape),))\n)\nax.plot([-5,5], [-5,5], 'k-')\nplt.title(\"Reconstructed vs Original data.\")\nplt.xlabel(\"Original data.\")\nplt.ylabel(\"Reconstructed data.\")\nplt.show()\n\n6/6 [==============================] - 0s 3ms/step\n6/6 [==============================] - 0s 2ms/step\n\n\n\n\n\nWe can also plot the estimated latent variables (the encoded data), which should approximately be standard normal random variables.\n\nplt.hist(encoded)\nplt.title(\"Empirical distribution of the estimated latent variables.\")\nplt.xlabel(\"Value of the estimated latent variable.\")\nplt.ylabel(\"Frequency\")\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\nFinally, we can also plot a selection of the \\(g_j\\) functions.\n\n# Define a range of values to plot\nz_grid = np.arange(-3,3, .1)\nz_grid = np.expand_dims(z_grid, axis=1)\n\n# Obtain the map of these values through the estimated g_j functions\ny_grid, _ = vae.decoder.predict(z_grid)\n\n# Plot 20 of these\nn = 20\nplt.figure()\nfor i in range(n):\n    plt.subplot(4,int(n/4),i+1)\n    plt.plot(z_grid[:,-1], y_grid[:,i, -1])\n\n2/2 [==============================] - 0s 5ms/step"
  },
  {
    "objectID": "posts/vae/vae.html#load-data",
    "href": "posts/vae/vae.html#load-data",
    "title": "VAE implementation for Nonlinar Latent Variable Modeling",
    "section": "",
    "text": "We load a toy dataset with dimensions \\(n=100, p=100\\).\n\ndata = pd.read_csv(\"./data/q1/data_1_200_100_1.csv\")\ndata = np.array(data, dtype='float32')\ndata = np.expand_dims(data, axis=-1)\ndata_train = data[:int(data.shape[0]*.9)]\ndata_test  = data[int(data.shape[0]*.9):]\n\n\ndata_train.shape\n\n(180, 100, 1)"
  },
  {
    "objectID": "posts/vae/vae.html#create-the-sampling-layer",
    "href": "posts/vae/vae.html#create-the-sampling-layer",
    "title": "VAE implementation for Nonlinar Latent Variable Modeling",
    "section": "",
    "text": "class Sampling(layers.Layer):\n    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
  },
  {
    "objectID": "posts/vae/vae.html#create-the-encoder",
    "href": "posts/vae/vae.html#create-the-encoder",
    "title": "VAE implementation for Nonlinar Latent Variable Modeling",
    "section": "",
    "text": "The encoder encodes the observed variables into the parameters of the posterior distribution of \\(Z^{(i)}|Y^{(i)}\\). Both the mean and variance share the first layer.\n\nlatent_dim = 1\n\nencoder_inputs = keras.Input(shape=data_train.shape[1:])\nx = layers.Flatten()(encoder_inputs)\nx = layers.Dense(100, activation=\"relu\")(x)\nz_mean = layers.Dense(100, activation=\"relu\")(x)\nz_mean = layers.Dense(100, activation=\"relu\")(x)\nz_mean =layers.Dense(latent_dim, name=\"z_mean\", activation=\"linear\")(z_mean)\n\nz_log_var = layers.Dense(100, activation=\"relu\")(x)\nz_log_var = layers.Dense(100, activation=\"relu\")(x)\nz_log_var = layers.Dense(latent_dim, name=\"z_log_var\", activation=\"linear\")(z_log_var)\nz = Sampling()([z_mean, z_log_var])\n\nencoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n\nencoder.summary()\n\nModel: \"encoder\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 100, 1)]     0           []                               \n                                                                                                  \n flatten (Flatten)              (None, 100)          0           ['input_1[0][0]']                \n                                                                                                  \n dense (Dense)                  (None, 100)          10100       ['flatten[0][0]']                \n                                                                                                  \n dense_2 (Dense)                (None, 100)          10100       ['dense[0][0]']                  \n                                                                                                  \n dense_4 (Dense)                (None, 100)          10100       ['dense[0][0]']                  \n                                                                                                  \n z_mean (Dense)                 (None, 1)            101         ['dense_2[0][0]']                \n                                                                                                  \n z_log_var (Dense)              (None, 1)            101         ['dense_4[0][0]']                \n                                                                                                  \n sampling (Sampling)            (None, 1)            0           ['z_mean[0][0]',                 \n                                                                  'z_log_var[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 30,502\nTrainable params: 30,502\nNon-trainable params: 0\n__________________________________________________________________________________________________"
  },
  {
    "objectID": "posts/vae/vae.html#create-the-decoder",
    "href": "posts/vae/vae.html#create-the-decoder",
    "title": "VAE implementation for Nonlinar Latent Variable Modeling",
    "section": "",
    "text": "The decoder has to be flexible enough to be able to model the functions \\(g_j\\), which models the conditional means of the responses. On top of that, the decoder models the residual variances \\(\\sigma_j\\).\n\nlatent_inputs = keras.Input(shape=(latent_dim,))\nx = layers.Dense(50, activation=\"relu\")(latent_inputs)\nx = layers.Dense(100, activation=\"relu\")(x)\n\nlog_var = layers.Dense(100, activation=\"relu\")(x)\nlog_var = layers.Dense(100, activation=\"linear\")(log_var)\nlog_var = layers.Reshape(data_train.shape[1:])(log_var)\n\n\nx_output = layers.Dense(100, activation=\"relu\")(x)\nx_output = layers.Dense(100, activation=\"linear\")(x_output)\nx_output = layers.Reshape(data_train.shape[1:])(x_output)\n\ndecoder = keras.Model(latent_inputs, [x_output, log_var], name=\"decoder\")\ndecoder.summary()\n\nModel: \"decoder\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_2 (InputLayer)           [(None, 1)]          0           []                               \n                                                                                                  \n dense_5 (Dense)                (None, 50)           100         ['input_2[0][0]']                \n                                                                                                  \n dense_6 (Dense)                (None, 100)          5100        ['dense_5[0][0]']                \n                                                                                                  \n dense_9 (Dense)                (None, 100)          10100       ['dense_6[0][0]']                \n                                                                                                  \n dense_7 (Dense)                (None, 100)          10100       ['dense_6[0][0]']                \n                                                                                                  \n dense_10 (Dense)               (None, 100)          10100       ['dense_9[0][0]']                \n                                                                                                  \n dense_8 (Dense)                (None, 100)          10100       ['dense_7[0][0]']                \n                                                                                                  \n reshape_1 (Reshape)            (None, 100, 1)       0           ['dense_10[0][0]']               \n                                                                                                  \n reshape (Reshape)              (None, 100, 1)       0           ['dense_8[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 45,600\nTrainable params: 45,600\nNon-trainable params: 0\n__________________________________________________________________________________________________"
  },
  {
    "objectID": "posts/vae/vae.html#create-the-variational-autoencoder",
    "href": "posts/vae/vae.html#create-the-variational-autoencoder",
    "title": "VAE implementation for Nonlinar Latent Variable Modeling",
    "section": "",
    "text": "The variational auto-encoder (VAE) is itself a keras.Model object. It consists of the encoder and decoder layers, a specialized train_step as well of some specialized metrics to keep track of our progress.\n\nclass VAE(keras.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super().__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = keras.metrics.Mean(\n            name=\"reconstruction_loss\"\n        )\n        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, z = self.encoder(data)\n            reconstruction, logvar = self.decoder(z)\n            logvar = tf.reduce_mean(logvar, axis=1, keepdims=True)\n\n            # Reconstruction loss (including the residual variances)\n            reconstruction_loss = 0.5 * tf.reduce_mean(tf.reduce_sum((data - reconstruction)**2 * tf.math.exp(-logvar) + logvar + tf.math.log(2. * np.pi), axis=1))\n\n            # ELBO loss, approximating the posterior with a Gaussian.\n            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n            total_loss = reconstruction_loss + kl_loss\n\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n        return {\n            \"total_loss\": self.total_loss_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n            \"kl_loss\": self.kl_loss_tracker.result(),\n        }\n\n\nvae = VAE(encoder, decoder)\nvae.compile(optimizer=keras.optimizers.Adam())\nvae.fit(data_train, epochs=200)\n\nEpoch 1/200\n6/6 [==============================] - 5s 5ms/step - total_loss: 159.4694 - reconstruction_loss: 158.8095 - kl_loss: 0.6599\nEpoch 2/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 157.3188 - reconstruction_loss: 156.3394 - kl_loss: 0.9794\nEpoch 3/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 154.5943 - reconstruction_loss: 153.0545 - kl_loss: 1.5398\nEpoch 4/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 152.1033 - reconstruction_loss: 150.5485 - kl_loss: 1.5548\nEpoch 5/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 147.7574 - reconstruction_loss: 144.8417 - kl_loss: 2.9157\nEpoch 6/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 142.2038 - reconstruction_loss: 138.8096 - kl_loss: 3.3943\nEpoch 7/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 136.3394 - reconstruction_loss: 133.4327 - kl_loss: 2.9067\nEpoch 8/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 132.0306 - reconstruction_loss: 129.1889 - kl_loss: 2.8417\nEpoch 9/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 128.4884 - reconstruction_loss: 125.7331 - kl_loss: 2.7553\nEpoch 10/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 126.0616 - reconstruction_loss: 123.3086 - kl_loss: 2.7530\nEpoch 11/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 123.9195 - reconstruction_loss: 121.1649 - kl_loss: 2.7547\nEpoch 12/200\n6/6 [==============================] - 0s 5ms/step - total_loss: 123.4685 - reconstruction_loss: 120.7576 - kl_loss: 2.7109\nEpoch 13/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 121.9943 - reconstruction_loss: 119.0857 - kl_loss: 2.9086\nEpoch 14/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 121.4584 - reconstruction_loss: 118.4306 - kl_loss: 3.0278\nEpoch 15/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 121.2427 - reconstruction_loss: 118.3778 - kl_loss: 2.8650\nEpoch 16/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 120.2809 - reconstruction_loss: 117.4173 - kl_loss: 2.8635\nEpoch 17/200\n6/6 [==============================] - 0s 5ms/step - total_loss: 119.4762 - reconstruction_loss: 116.6165 - kl_loss: 2.8597\nEpoch 18/200\n6/6 [==============================] - 0s 5ms/step - total_loss: 118.1459 - reconstruction_loss: 115.3286 - kl_loss: 2.8172\nEpoch 19/200\n6/6 [==============================] - 0s 13ms/step - total_loss: 117.4240 - reconstruction_loss: 114.6200 - kl_loss: 2.8040\nEpoch 20/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 117.1775 - reconstruction_loss: 114.3382 - kl_loss: 2.8392\nEpoch 21/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 116.7512 - reconstruction_loss: 113.8932 - kl_loss: 2.8580\nEpoch 22/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 114.6956 - reconstruction_loss: 111.8335 - kl_loss: 2.8621\nEpoch 23/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 114.0700 - reconstruction_loss: 111.2253 - kl_loss: 2.8447\nEpoch 24/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 111.6568 - reconstruction_loss: 108.7404 - kl_loss: 2.9164\nEpoch 25/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 110.1693 - reconstruction_loss: 107.2955 - kl_loss: 2.8737\nEpoch 26/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 108.0621 - reconstruction_loss: 105.1189 - kl_loss: 2.9431\nEpoch 27/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 105.9520 - reconstruction_loss: 102.7703 - kl_loss: 3.1818\nEpoch 28/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 104.2761 - reconstruction_loss: 101.0551 - kl_loss: 3.2210\nEpoch 29/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 102.5817 - reconstruction_loss: 99.3796 - kl_loss: 3.2020\nEpoch 30/200\n6/6 [==============================] - 0s 18ms/step - total_loss: 100.3066 - reconstruction_loss: 97.1519 - kl_loss: 3.1548\nEpoch 31/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 99.7117 - reconstruction_loss: 96.4821 - kl_loss: 3.2297\nEpoch 32/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 97.7306 - reconstruction_loss: 94.2878 - kl_loss: 3.4428\nEpoch 33/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 96.3645 - reconstruction_loss: 92.9167 - kl_loss: 3.4479\nEpoch 34/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 95.0677 - reconstruction_loss: 91.7078 - kl_loss: 3.3598\nEpoch 35/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 94.1247 - reconstruction_loss: 90.6508 - kl_loss: 3.4740\nEpoch 36/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 94.0376 - reconstruction_loss: 90.4024 - kl_loss: 3.6352\nEpoch 37/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 92.4411 - reconstruction_loss: 88.6930 - kl_loss: 3.7481\nEpoch 38/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 91.0048 - reconstruction_loss: 87.3415 - kl_loss: 3.6633\nEpoch 39/200\n6/6 [==============================] - 0s 30ms/step - total_loss: 89.7175 - reconstruction_loss: 86.2138 - kl_loss: 3.5037\nEpoch 40/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 89.1252 - reconstruction_loss: 85.4268 - kl_loss: 3.6984\nEpoch 41/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 87.8362 - reconstruction_loss: 83.8817 - kl_loss: 3.9545\nEpoch 42/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 87.6689 - reconstruction_loss: 83.7613 - kl_loss: 3.9076\nEpoch 43/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 87.3709 - reconstruction_loss: 83.5801 - kl_loss: 3.7908\nEpoch 44/200\n6/6 [==============================] - 0s 17ms/step - total_loss: 85.8306 - reconstruction_loss: 81.8511 - kl_loss: 3.9795\nEpoch 45/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 85.7300 - reconstruction_loss: 81.7236 - kl_loss: 4.0064\nEpoch 46/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 85.5675 - reconstruction_loss: 81.6855 - kl_loss: 3.8820\nEpoch 47/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 85.3483 - reconstruction_loss: 81.4715 - kl_loss: 3.8768\nEpoch 48/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 85.4788 - reconstruction_loss: 81.6757 - kl_loss: 3.8031\nEpoch 49/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 84.3944 - reconstruction_loss: 80.5271 - kl_loss: 3.8674\nEpoch 50/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 83.8853 - reconstruction_loss: 79.8080 - kl_loss: 4.0773\nEpoch 51/200\n6/6 [==============================] - 0s 14ms/step - total_loss: 83.1672 - reconstruction_loss: 79.2457 - kl_loss: 3.9215\nEpoch 52/200\n6/6 [==============================] - 0s 20ms/step - total_loss: 83.2434 - reconstruction_loss: 79.4520 - kl_loss: 3.7914\nEpoch 53/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 83.0088 - reconstruction_loss: 79.0819 - kl_loss: 3.9269\nEpoch 54/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 81.8423 - reconstruction_loss: 77.8640 - kl_loss: 3.9783\nEpoch 55/200\n6/6 [==============================] - 0s 14ms/step - total_loss: 81.7239 - reconstruction_loss: 77.8502 - kl_loss: 3.8737\nEpoch 56/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 81.2914 - reconstruction_loss: 77.5255 - kl_loss: 3.7659\nEpoch 57/200\n6/6 [==============================] - 0s 24ms/step - total_loss: 81.2742 - reconstruction_loss: 77.5173 - kl_loss: 3.7568\nEpoch 58/200\n6/6 [==============================] - 0s 16ms/step - total_loss: 81.4382 - reconstruction_loss: 77.6066 - kl_loss: 3.8316\nEpoch 59/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 81.4028 - reconstruction_loss: 77.5080 - kl_loss: 3.8948\nEpoch 60/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 81.1439 - reconstruction_loss: 77.1630 - kl_loss: 3.9810\nEpoch 61/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 80.4591 - reconstruction_loss: 76.5975 - kl_loss: 3.8615\nEpoch 62/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 80.4920 - reconstruction_loss: 76.7512 - kl_loss: 3.7409\nEpoch 63/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 80.4919 - reconstruction_loss: 76.6208 - kl_loss: 3.8711\nEpoch 64/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 80.5306 - reconstruction_loss: 76.4850 - kl_loss: 4.0457\nEpoch 65/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 80.0165 - reconstruction_loss: 76.0480 - kl_loss: 3.9685\nEpoch 66/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 80.0417 - reconstruction_loss: 76.3429 - kl_loss: 3.6988\nEpoch 67/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 79.9329 - reconstruction_loss: 76.2699 - kl_loss: 3.6630\nEpoch 68/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 79.9716 - reconstruction_loss: 76.1393 - kl_loss: 3.8324\nEpoch 69/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 80.6046 - reconstruction_loss: 76.6400 - kl_loss: 3.9647\nEpoch 70/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 79.8648 - reconstruction_loss: 75.8009 - kl_loss: 4.0638\nEpoch 71/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 79.7548 - reconstruction_loss: 75.7707 - kl_loss: 3.9841\nEpoch 72/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 79.5776 - reconstruction_loss: 75.6953 - kl_loss: 3.8823\nEpoch 73/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 79.5520 - reconstruction_loss: 75.6548 - kl_loss: 3.8971\nEpoch 74/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 79.5209 - reconstruction_loss: 75.5917 - kl_loss: 3.9292\nEpoch 75/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 79.3321 - reconstruction_loss: 75.3729 - kl_loss: 3.9591\nEpoch 76/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 78.8106 - reconstruction_loss: 74.7718 - kl_loss: 4.0388\nEpoch 77/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 78.3486 - reconstruction_loss: 74.4723 - kl_loss: 3.8763\nEpoch 78/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 78.4589 - reconstruction_loss: 74.6674 - kl_loss: 3.7914\nEpoch 79/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 78.1411 - reconstruction_loss: 74.1538 - kl_loss: 3.9873\nEpoch 80/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 77.9273 - reconstruction_loss: 73.9364 - kl_loss: 3.9909\nEpoch 81/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 77.6897 - reconstruction_loss: 73.8694 - kl_loss: 3.8203\nEpoch 82/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 77.6744 - reconstruction_loss: 73.9221 - kl_loss: 3.7522\nEpoch 83/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 77.5535 - reconstruction_loss: 73.8011 - kl_loss: 3.7523\nEpoch 84/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 77.9563 - reconstruction_loss: 74.0288 - kl_loss: 3.9275\nEpoch 85/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 77.7252 - reconstruction_loss: 73.7183 - kl_loss: 4.0069\nEpoch 86/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 77.2565 - reconstruction_loss: 73.4258 - kl_loss: 3.8307\nEpoch 87/200\n6/6 [==============================] - 0s 18ms/step - total_loss: 78.0296 - reconstruction_loss: 74.2869 - kl_loss: 3.7428\nEpoch 88/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 77.7511 - reconstruction_loss: 73.7531 - kl_loss: 3.9980\nEpoch 89/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 77.2437 - reconstruction_loss: 73.1683 - kl_loss: 4.0753\nEpoch 90/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 77.6239 - reconstruction_loss: 73.6363 - kl_loss: 3.9876\nEpoch 91/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 76.9329 - reconstruction_loss: 73.0359 - kl_loss: 3.8971\nEpoch 92/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 77.0101 - reconstruction_loss: 73.1212 - kl_loss: 3.8888\nEpoch 93/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 76.9049 - reconstruction_loss: 72.9713 - kl_loss: 3.9337\nEpoch 94/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 76.8861 - reconstruction_loss: 72.9856 - kl_loss: 3.9005\nEpoch 95/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 76.7297 - reconstruction_loss: 72.8783 - kl_loss: 3.8513\nEpoch 96/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 76.8668 - reconstruction_loss: 73.0576 - kl_loss: 3.8092\nEpoch 97/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 77.0898 - reconstruction_loss: 73.2349 - kl_loss: 3.8549\nEpoch 98/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 76.6132 - reconstruction_loss: 72.7206 - kl_loss: 3.8926\nEpoch 99/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 77.0141 - reconstruction_loss: 73.0733 - kl_loss: 3.9408\nEpoch 100/200\n6/6 [==============================] - 0s 13ms/step - total_loss: 76.4530 - reconstruction_loss: 72.4564 - kl_loss: 3.9967\nEpoch 101/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 76.6080 - reconstruction_loss: 72.6005 - kl_loss: 4.0075\nEpoch 102/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 76.6930 - reconstruction_loss: 72.6261 - kl_loss: 4.0669\nEpoch 103/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 76.7384 - reconstruction_loss: 72.8002 - kl_loss: 3.9383\nEpoch 104/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 76.4935 - reconstruction_loss: 72.6563 - kl_loss: 3.8372\nEpoch 105/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 76.3498 - reconstruction_loss: 72.5485 - kl_loss: 3.8012\nEpoch 106/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 76.2614 - reconstruction_loss: 72.3045 - kl_loss: 3.9570\nEpoch 107/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 76.1234 - reconstruction_loss: 72.1581 - kl_loss: 3.9653\nEpoch 108/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 76.1155 - reconstruction_loss: 72.2819 - kl_loss: 3.8336\nEpoch 109/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.7346 - reconstruction_loss: 71.9616 - kl_loss: 3.7731\nEpoch 110/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 76.0551 - reconstruction_loss: 72.2147 - kl_loss: 3.8403\nEpoch 111/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 76.6325 - reconstruction_loss: 72.6785 - kl_loss: 3.9540\nEpoch 112/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 76.2791 - reconstruction_loss: 72.3832 - kl_loss: 3.8959\nEpoch 113/200\n6/6 [==============================] - 0s 14ms/step - total_loss: 76.3594 - reconstruction_loss: 72.5248 - kl_loss: 3.8346\nEpoch 114/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.7003 - reconstruction_loss: 71.9479 - kl_loss: 3.7525\nEpoch 115/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 75.8326 - reconstruction_loss: 72.0700 - kl_loss: 3.7626\nEpoch 116/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.8259 - reconstruction_loss: 71.8656 - kl_loss: 3.9603\nEpoch 117/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.9852 - reconstruction_loss: 72.0045 - kl_loss: 3.9808\nEpoch 118/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.5266 - reconstruction_loss: 71.6431 - kl_loss: 3.8835\nEpoch 119/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 75.8531 - reconstruction_loss: 71.9800 - kl_loss: 3.8731\nEpoch 120/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.4883 - reconstruction_loss: 71.5299 - kl_loss: 3.9584\nEpoch 121/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 75.7388 - reconstruction_loss: 71.7915 - kl_loss: 3.9474\nEpoch 122/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.5227 - reconstruction_loss: 71.7053 - kl_loss: 3.8173\nEpoch 123/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.2283 - reconstruction_loss: 71.4103 - kl_loss: 3.8180\nEpoch 124/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.4891 - reconstruction_loss: 71.6811 - kl_loss: 3.8079\nEpoch 125/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.7789 - reconstruction_loss: 71.9703 - kl_loss: 3.8086\nEpoch 126/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.8799 - reconstruction_loss: 72.0370 - kl_loss: 3.8429\nEpoch 127/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.8861 - reconstruction_loss: 71.9968 - kl_loss: 3.8893\nEpoch 128/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.2229 - reconstruction_loss: 71.3336 - kl_loss: 3.8892\nEpoch 129/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.5182 - reconstruction_loss: 71.6709 - kl_loss: 3.8473\nEpoch 130/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.4523 - reconstruction_loss: 71.6015 - kl_loss: 3.8508\nEpoch 131/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 75.1373 - reconstruction_loss: 71.2577 - kl_loss: 3.8796\nEpoch 132/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.0030 - reconstruction_loss: 71.1554 - kl_loss: 3.8476\nEpoch 133/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 75.2704 - reconstruction_loss: 71.5029 - kl_loss: 3.7676\nEpoch 134/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.1937 - reconstruction_loss: 71.4221 - kl_loss: 3.7716\nEpoch 135/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.2062 - reconstruction_loss: 71.4695 - kl_loss: 3.7366\nEpoch 136/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 75.5514 - reconstruction_loss: 71.7997 - kl_loss: 3.7517\nEpoch 137/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 75.3288 - reconstruction_loss: 71.5681 - kl_loss: 3.7607\nEpoch 138/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.8560 - reconstruction_loss: 71.0130 - kl_loss: 3.8430\nEpoch 139/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.8883 - reconstruction_loss: 71.1001 - kl_loss: 3.7883\nEpoch 140/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 75.0647 - reconstruction_loss: 71.2895 - kl_loss: 3.7752\nEpoch 141/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 74.9836 - reconstruction_loss: 71.2304 - kl_loss: 3.7532\nEpoch 142/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.1171 - reconstruction_loss: 71.3837 - kl_loss: 3.7335\nEpoch 143/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 74.6719 - reconstruction_loss: 70.7620 - kl_loss: 3.9100\nEpoch 144/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 74.9929 - reconstruction_loss: 71.1038 - kl_loss: 3.8891\nEpoch 145/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 74.9700 - reconstruction_loss: 71.1671 - kl_loss: 3.8029\nEpoch 146/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.7118 - reconstruction_loss: 71.0037 - kl_loss: 3.7081\nEpoch 147/200\n6/6 [==============================] - 0s 13ms/step - total_loss: 74.7556 - reconstruction_loss: 70.9940 - kl_loss: 3.7616\nEpoch 148/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 75.2549 - reconstruction_loss: 71.4497 - kl_loss: 3.8052\nEpoch 149/200\n6/6 [==============================] - 0s 6ms/step - total_loss: 75.3068 - reconstruction_loss: 71.4709 - kl_loss: 3.8358\nEpoch 150/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.5678 - reconstruction_loss: 71.8368 - kl_loss: 3.7310\nEpoch 151/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.6995 - reconstruction_loss: 71.8900 - kl_loss: 3.8095\nEpoch 152/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.5212 - reconstruction_loss: 71.6031 - kl_loss: 3.9180\nEpoch 153/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.4468 - reconstruction_loss: 71.5346 - kl_loss: 3.9122\nEpoch 154/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.0431 - reconstruction_loss: 71.1578 - kl_loss: 3.8854\nEpoch 155/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 74.8433 - reconstruction_loss: 71.0172 - kl_loss: 3.8261\nEpoch 156/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 75.0288 - reconstruction_loss: 71.2482 - kl_loss: 3.7805\nEpoch 157/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 74.8153 - reconstruction_loss: 70.9827 - kl_loss: 3.8326\nEpoch 158/200\n6/6 [==============================] - 0s 13ms/step - total_loss: 75.0699 - reconstruction_loss: 71.2169 - kl_loss: 3.8530\nEpoch 159/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 74.8352 - reconstruction_loss: 71.0610 - kl_loss: 3.7742\nEpoch 160/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.0455 - reconstruction_loss: 71.2989 - kl_loss: 3.7466\nEpoch 161/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 74.9925 - reconstruction_loss: 71.2544 - kl_loss: 3.7381\nEpoch 162/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 75.1559 - reconstruction_loss: 71.4062 - kl_loss: 3.7497\nEpoch 163/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 74.7373 - reconstruction_loss: 70.9925 - kl_loss: 3.7449\nEpoch 164/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 75.0982 - reconstruction_loss: 71.3413 - kl_loss: 3.7569\nEpoch 165/200\n6/6 [==============================] - 0s 14ms/step - total_loss: 74.7588 - reconstruction_loss: 70.9353 - kl_loss: 3.8235\nEpoch 166/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 75.2021 - reconstruction_loss: 71.3836 - kl_loss: 3.8186\nEpoch 167/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 74.8396 - reconstruction_loss: 70.9573 - kl_loss: 3.8823\nEpoch 168/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 74.9674 - reconstruction_loss: 71.1168 - kl_loss: 3.8506\nEpoch 169/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 75.1807 - reconstruction_loss: 71.3710 - kl_loss: 3.8097\nEpoch 170/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 74.8106 - reconstruction_loss: 71.0109 - kl_loss: 3.7997\nEpoch 171/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 75.7396 - reconstruction_loss: 71.8767 - kl_loss: 3.8629\nEpoch 172/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.0245 - reconstruction_loss: 71.1096 - kl_loss: 3.9150\nEpoch 173/200\n6/6 [==============================] - 0s 12ms/step - total_loss: 74.6527 - reconstruction_loss: 70.8753 - kl_loss: 3.7774\nEpoch 174/200\n6/6 [==============================] - 0s 15ms/step - total_loss: 74.5188 - reconstruction_loss: 70.8158 - kl_loss: 3.7029\nEpoch 175/200\n6/6 [==============================] - 0s 13ms/step - total_loss: 74.2905 - reconstruction_loss: 70.5769 - kl_loss: 3.7136\nEpoch 176/200\n6/6 [==============================] - 0s 14ms/step - total_loss: 74.4287 - reconstruction_loss: 70.6487 - kl_loss: 3.7800\nEpoch 177/200\n6/6 [==============================] - 0s 14ms/step - total_loss: 74.2941 - reconstruction_loss: 70.5216 - kl_loss: 3.7725\nEpoch 178/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 74.5420 - reconstruction_loss: 70.7651 - kl_loss: 3.7769\nEpoch 179/200\n6/6 [==============================] - 0s 20ms/step - total_loss: 74.3038 - reconstruction_loss: 70.4219 - kl_loss: 3.8819\nEpoch 180/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 74.5688 - reconstruction_loss: 70.7080 - kl_loss: 3.8608\nEpoch 181/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 74.5755 - reconstruction_loss: 70.7965 - kl_loss: 3.7790\nEpoch 182/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 74.8235 - reconstruction_loss: 71.1014 - kl_loss: 3.7220\nEpoch 183/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.3601 - reconstruction_loss: 70.6418 - kl_loss: 3.7183\nEpoch 184/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.7422 - reconstruction_loss: 71.0048 - kl_loss: 3.7374\nEpoch 185/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 74.5962 - reconstruction_loss: 70.8452 - kl_loss: 3.7511\nEpoch 186/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 74.4384 - reconstruction_loss: 70.7097 - kl_loss: 3.7287\nEpoch 187/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.6677 - reconstruction_loss: 70.8648 - kl_loss: 3.8030\nEpoch 188/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.7798 - reconstruction_loss: 70.9518 - kl_loss: 3.8280\nEpoch 189/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.7135 - reconstruction_loss: 70.9471 - kl_loss: 3.7664\nEpoch 190/200\n6/6 [==============================] - 0s 11ms/step - total_loss: 74.9372 - reconstruction_loss: 71.2350 - kl_loss: 3.7022\nEpoch 191/200\n6/6 [==============================] - 0s 8ms/step - total_loss: 75.1655 - reconstruction_loss: 71.3957 - kl_loss: 3.7698\nEpoch 192/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.6117 - reconstruction_loss: 70.6775 - kl_loss: 3.9343\nEpoch 193/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 74.9547 - reconstruction_loss: 71.0987 - kl_loss: 3.8560\nEpoch 194/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 75.1745 - reconstruction_loss: 71.3602 - kl_loss: 3.8143\nEpoch 195/200\n6/6 [==============================] - 0s 7ms/step - total_loss: 74.7096 - reconstruction_loss: 70.9043 - kl_loss: 3.8053\nEpoch 196/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 75.4985 - reconstruction_loss: 71.7083 - kl_loss: 3.7902\nEpoch 197/200\n6/6 [==============================] - 0s 9ms/step - total_loss: 75.2293 - reconstruction_loss: 71.4085 - kl_loss: 3.8209\nEpoch 198/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 74.9497 - reconstruction_loss: 71.1077 - kl_loss: 3.8421\nEpoch 199/200\n6/6 [==============================] - 0s 14ms/step - total_loss: 74.9374 - reconstruction_loss: 71.1824 - kl_loss: 3.7549\nEpoch 200/200\n6/6 [==============================] - 0s 10ms/step - total_loss: 74.5571 - reconstruction_loss: 70.8363 - kl_loss: 3.7208"
  },
  {
    "objectID": "posts/vae/vae.html#plot",
    "href": "posts/vae/vae.html#plot",
    "title": "VAE implementation for Nonlinar Latent Variable Modeling",
    "section": "",
    "text": "We now plot the fit, i.e.Â the reconstruction vs the original data.\n\nencoded, _, _ = vae.encoder.predict(data_train)\ndecoded, var = vae.decoder.predict(encoded)\n\nf, ax = plt.subplots()\nax.scatter(\n    data_train.reshape((np.prod(decoded.shape),)),\n    decoded.reshape((np.prod(decoded.shape),))\n)\nax.plot([-5,5], [-5,5], 'k-')\nplt.title(\"Reconstructed vs Original data.\")\nplt.xlabel(\"Original data.\")\nplt.ylabel(\"Reconstructed data.\")\nplt.show()\n\n6/6 [==============================] - 0s 3ms/step\n6/6 [==============================] - 0s 2ms/step\n\n\n\n\n\nWe can also plot the estimated latent variables (the encoded data), which should approximately be standard normal random variables.\n\nplt.hist(encoded)\nplt.title(\"Empirical distribution of the estimated latent variables.\")\nplt.xlabel(\"Value of the estimated latent variable.\")\nplt.ylabel(\"Frequency\")\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\nFinally, we can also plot a selection of the \\(g_j\\) functions.\n\n# Define a range of values to plot\nz_grid = np.arange(-3,3, .1)\nz_grid = np.expand_dims(z_grid, axis=1)\n\n# Obtain the map of these values through the estimated g_j functions\ny_grid, _ = vae.decoder.predict(z_grid)\n\n# Plot 20 of these\nn = 20\nplt.figure()\nfor i in range(n):\n    plt.subplot(4,int(n/4),i+1)\n    plt.plot(z_grid[:,-1], y_grid[:,i, -1])\n\n2/2 [==============================] - 0s 5ms/step"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here will be my various data-science projects"
  }
]